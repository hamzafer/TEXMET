{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "25754b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempt 1 failed with status 403. Retrying in 2 seconds...\n",
      "Attempt 2 failed with status 403. Retrying in 2 seconds...\n",
      "Attempt 3 failed with status 403. Retrying in 2 seconds...\n",
      "Found 33437 textile objects\n",
      "Object IDs saved to textile_object_ids.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "max_retries = 5\n",
    "retry_delay = 2  # seconds\n",
    "\n",
    "for attempt in range(max_retries):\n",
    "    response = requests.get(search_url, params=params)\n",
    "    if response.status_code == 200:\n",
    "        break\n",
    "    print(f\"Attempt {attempt + 1} failed with status {response.status_code}. Retrying in {retry_delay} seconds...\")\n",
    "    time.sleep(retry_delay)\n",
    "else:\n",
    "    raise Exception(f\"Failed to fetch data after {max_retries} attempts. Last status: {response.status_code}\")\n",
    "\n",
    "search_data = response.json()\n",
    "object_ids = search_data[\"objectIDs\"]\n",
    "print(f\"Found {len(object_ids)} textile objects\")\n",
    "\n",
    "with open(\"textile_object_ids.json\", \"w\") as f:\n",
    "    json.dump(object_ids, f)\n",
    "print(\"Object IDs saved to textile_object_ids.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b2b0ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 33437 object IDs from JSON.\n",
      "Verification: True\n"
     ]
    }
   ],
   "source": [
    "with open(\"textile_object_ids.json\", \"r\") as f_read:\n",
    "    loaded_object_ids = json.load(f_read)\n",
    "\n",
    "# Verify that the loaded IDs match the original object_ids\n",
    "print(f\"Loaded {len(loaded_object_ids)} object IDs from JSON.\")\n",
    "print(\"Verification:\", loaded_object_ids == object_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "57cd00d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted 22054 object IDs from batch.\n",
      "Match with loaded_object_ids: False\n"
     ]
    }
   ],
   "source": [
    "batch_object_ids_extracted = [obj[\"objectID\"] for obj in batch_object_ids]\n",
    "print(f\"Extracted {len(batch_object_ids_extracted)} object IDs from batch.\")\n",
    "print(\"Match with loaded_object_ids:\", set(batch_object_ids_extracted) == set(loaded_object_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01820915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total textile objects: 33437\n",
      "Objects in your JSON file: 22104\n",
      "Objects with images: 17749\n",
      "\n",
      "=== PROGRESS SUMMARY ===\n",
      "✅ Downloaded: 22104\n",
      "⏳ Remaining: 11333\n",
      "📊 Progress: 66.1%\n",
      "\n",
      "🎯 You have 11333 objects left to download\n",
      "📋 Sample remaining IDs: [13737, 13740, 13748, 13795, 13798, 13801, 14054, 14056, 14081, 14086]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the complete object ID list\n",
    "with open(\"textile_object_ids.json\", \"r\") as f:\n",
    "    all_object_ids = json.load(f)\n",
    "\n",
    "total_objects = len(all_object_ids)\n",
    "print(f\"Total textile objects: {total_objects}\")\n",
    "\n",
    "# Load your downloaded JSON file\n",
    "json_file = 'met_textiles_batch_22800_20250705_134702.json'\n",
    "\n",
    "try:\n",
    "    with open(json_file, 'r', encoding='utf-8') as f:\n",
    "        downloaded_data = json.load(f)\n",
    "    \n",
    "    downloaded_object_ids = set()\n",
    "    objects_with_images = 0\n",
    "    \n",
    "    for obj in downloaded_data:\n",
    "        if isinstance(obj, dict) and 'objectID' in obj:\n",
    "            downloaded_object_ids.add(obj['objectID'])\n",
    "            if obj.get('primaryImage'):\n",
    "                objects_with_images += 1\n",
    "    \n",
    "    print(f\"Objects in your JSON file: {len(downloaded_object_ids)}\")\n",
    "    print(f\"Objects with images: {objects_with_images}\")\n",
    "    \n",
    "    # Calculate what's left\n",
    "    all_object_ids_set = set(all_object_ids)\n",
    "    remaining_ids = all_object_ids_set - downloaded_object_ids\n",
    "    completed_ids = all_object_ids_set & downloaded_object_ids\n",
    "    \n",
    "    print(f\"\\n=== PROGRESS SUMMARY ===\")\n",
    "    print(f\"✅ Downloaded: {len(completed_ids)}\")\n",
    "    print(f\"⏳ Remaining: {len(remaining_ids)}\")\n",
    "    print(f\"📊 Progress: {len(completed_ids)/total_objects*100:.1f}%\")\n",
    "    \n",
    "    if len(remaining_ids) > 0:\n",
    "        print(f\"\\n🎯 You have {len(remaining_ids)} objects left to download\")\n",
    "        print(f\"📋 Sample remaining IDs: {sorted(list(remaining_ids))[:10]}\")\n",
    "    else:\n",
    "        print(f\"\\n🎉 All objects downloaded!\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading JSON file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae8d6784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total textile objects: 33437\n",
      "\n",
      "=== CHECKING FORWARD DOWNLOAD ===\n",
      "✅ Forward file: 22104 objects, 17749 with images\n",
      "\n",
      "=== CHECKING REVERSE DOWNLOAD ===\n",
      "✅ Reverse file: 20497 objects, 16850 with images\n",
      "\n",
      "=== OVERLAP ANALYSIS ===\n",
      "🔄 Duplicate objects: 10143\n",
      "📋 Sample duplicates: [13561, 13562, 13794, 13796, 13799, 13800, 13838, 14186, 21466, 21946]\n",
      "\n",
      "=== FINAL PROGRESS SUMMARY ===\n",
      "📁 Forward file objects: 22104\n",
      "📁 Reverse file objects: 20497\n",
      "🔄 Duplicate objects: 10143\n",
      "✅ Total unique downloaded: 32458\n",
      "🖼️  Total objects with images: 26624\n",
      "⏳ Remaining to download: 979\n",
      "📊 Overall progress: 97.1%\n",
      "\n",
      "🎯 You still need 979 objects\n",
      "📋 Sample remaining IDs: [21107, 21919, 22276, 22739, 22874, 24007, 24647, 25030, 28904, 30925]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Load the complete object ID list\n",
    "with open(\"textile_object_ids.json\", \"r\") as f:\n",
    "    all_object_ids = json.load(f)\n",
    "\n",
    "total_objects = len(all_object_ids)\n",
    "print(f\"Total textile objects: {total_objects}\")\n",
    "\n",
    "# Load both downloaded JSON files\n",
    "forward_file = 'met_textiles_batch_22800_20250705_134702.json'\n",
    "reverse_file = 'idun/met_textiles_batch_11988_20250705_134921.json'\n",
    "\n",
    "all_downloaded_ids = set()\n",
    "total_objects_with_images = 0\n",
    "forward_count = 0\n",
    "reverse_count = 0\n",
    "\n",
    "print(\"\\n=== CHECKING FORWARD DOWNLOAD ===\")\n",
    "try:\n",
    "    with open(forward_file, 'r', encoding='utf-8') as f:\n",
    "        forward_data = json.load(f)\n",
    "    \n",
    "    forward_ids = set()\n",
    "    forward_images = 0\n",
    "    \n",
    "    for obj in forward_data:\n",
    "        if isinstance(obj, dict) and 'objectID' in obj:\n",
    "            forward_ids.add(obj['objectID'])\n",
    "            all_downloaded_ids.add(obj['objectID'])\n",
    "            if obj.get('primaryImage'):\n",
    "                forward_images += 1\n",
    "                total_objects_with_images += 1\n",
    "    \n",
    "    forward_count = len(forward_ids)\n",
    "    print(f\"✅ Forward file: {forward_count} objects, {forward_images} with images\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading forward file: {e}\")\n",
    "\n",
    "print(\"\\n=== CHECKING REVERSE DOWNLOAD ===\")\n",
    "try:\n",
    "    with open(reverse_file, 'r', encoding='utf-8') as f:\n",
    "        reverse_data = json.load(f)\n",
    "    \n",
    "    reverse_ids = set()\n",
    "    reverse_images = 0\n",
    "    \n",
    "    for obj in reverse_data:\n",
    "        if isinstance(obj, dict) and 'objectID' in obj:\n",
    "            reverse_ids.add(obj['objectID'])\n",
    "            if obj['objectID'] not in all_downloaded_ids:  # Avoid double counting images\n",
    "                all_downloaded_ids.add(obj['objectID'])\n",
    "                if obj.get('primaryImage'):\n",
    "                    total_objects_with_images += 1\n",
    "            if obj.get('primaryImage'):\n",
    "                reverse_images += 1\n",
    "    \n",
    "    reverse_count = len(reverse_ids)\n",
    "    print(f\"✅ Reverse file: {reverse_count} objects, {reverse_images} with images\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading reverse file: {e}\")\n",
    "\n",
    "# Check for overlap/duplicates\n",
    "if forward_count > 0 and reverse_count > 0:\n",
    "    overlap = forward_ids & reverse_ids\n",
    "    print(f\"\\n=== OVERLAP ANALYSIS ===\")\n",
    "    print(f\"🔄 Duplicate objects: {len(overlap)}\")\n",
    "    if len(overlap) > 0:\n",
    "        print(f\"📋 Sample duplicates: {sorted(list(overlap))[:10]}\")\n",
    "\n",
    "# Calculate final progress\n",
    "all_object_ids_set = set(all_object_ids)\n",
    "remaining_ids = all_object_ids_set - all_downloaded_ids\n",
    "completed_ids = all_object_ids_set & all_downloaded_ids\n",
    "\n",
    "print(f\"\\n=== FINAL PROGRESS SUMMARY ===\")\n",
    "print(f\"📁 Forward file objects: {forward_count}\")\n",
    "print(f\"📁 Reverse file objects: {reverse_count}\")\n",
    "print(f\"🔄 Duplicate objects: {len(overlap) if 'overlap' in locals() else 0}\")\n",
    "print(f\"✅ Total unique downloaded: {len(all_downloaded_ids)}\")\n",
    "print(f\"🖼️  Total objects with images: {total_objects_with_images}\")\n",
    "print(f\"⏳ Remaining to download: {len(remaining_ids)}\")\n",
    "print(f\"📊 Overall progress: {len(completed_ids)/total_objects*100:.1f}%\")\n",
    "\n",
    "if len(remaining_ids) > 0:\n",
    "    print(f\"\\n🎯 You still need {len(remaining_ids)} objects\")\n",
    "    print(f\"📋 Sample remaining IDs: {sorted(list(remaining_ids))[:10]}\")\n",
    "else:\n",
    "    print(f\"\\n🎉 ALL OBJECTS DOWNLOADED! 🎉\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "627a532d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 22104 IDs from forward file\n",
      "Total unique IDs after reverse: 32458\n",
      "Found 979 remaining objects to download\n",
      "Starting download of 979 remaining objects...\n",
      "Log file: remaining_download_log_20250705_135732.txt\n",
      "Progress: 1/979 (0.1%) - Rate: 0.0 req/sec\n",
      "Progress: 11/979 (1.1%) - Rate: 2.0 req/sec\n",
      "Progress: 21/979 (2.1%) - Rate: 2.1 req/sec\n",
      "Progress: 31/979 (3.2%) - Rate: 2.1 req/sec\n",
      "Rate limited on object 226347, waiting 10 seconds...\n",
      "Rate limited on object 226347, waiting 10 seconds...\n",
      "Rate limited on object 226347, waiting 10 seconds...\n",
      "Rate limited on object 226347, waiting 10 seconds...\n",
      "Rate limited on object 223360, waiting 10 seconds...\n",
      "Progress: 41/979 (4.2%) - Rate: 0.6 req/sec\n",
      "Saved batch to remaining_textiles_batch_50_20250705_135846.json\n",
      "Progress: 51/979 (5.2%) - Rate: 0.7 req/sec\n",
      "Rate limited on object 841930, waiting 10 seconds...\n",
      "Rate limited on object 841930, waiting 10 seconds...\n",
      "Rate limited on object 841930, waiting 10 seconds...\n",
      "Rate limited on object 841930, waiting 10 seconds...\n",
      "Progress: 61/979 (6.2%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 225491, waiting 10 seconds...\n",
      "Progress: 71/979 (7.3%) - Rate: 0.5 req/sec\n",
      "Progress: 81/979 (8.3%) - Rate: 0.6 req/sec\n",
      "Rate limited on object 760057, waiting 10 seconds...\n",
      "Rate limited on object 760057, waiting 10 seconds...\n",
      "Rate limited on object 760057, waiting 10 seconds...\n",
      "Rate limited on object 760057, waiting 10 seconds...\n",
      "Rate limited on object 760059, waiting 10 seconds...\n",
      "Progress: 91/979 (9.3%) - Rate: 0.5 req/sec\n",
      "Saved batch to remaining_textiles_batch_100_20250705_140051.json\n",
      "Progress: 101/979 (10.3%) - Rate: 0.5 req/sec\n",
      "Progress: 111/979 (11.3%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 225601, waiting 10 seconds...\n",
      "Rate limited on object 225601, waiting 10 seconds...\n",
      "Rate limited on object 222317, waiting 10 seconds...\n",
      "Rate limited on object 227657, waiting 10 seconds...\n",
      "Rate limited on object 22874, waiting 10 seconds...\n",
      "Progress: 121/979 (12.4%) - Rate: 0.5 req/sec\n",
      "Progress: 131/979 (13.4%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 72080, waiting 10 seconds...\n",
      "Rate limited on object 72080, waiting 10 seconds...\n",
      "Rate limited on object 696723, waiting 10 seconds...\n",
      "Rate limited on object 573842, waiting 10 seconds...\n",
      "Progress: 141/979 (14.4%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 319903, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_150_20250705_140255.json\n",
      "Progress: 151/979 (15.4%) - Rate: 0.5 req/sec\n",
      "Progress: 161/979 (16.4%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 696821, waiting 10 seconds...\n",
      "Rate limited on object 696821, waiting 10 seconds...\n",
      "Rate limited on object 696822, waiting 10 seconds...\n",
      "Rate limited on object 215914, waiting 10 seconds...\n",
      "Progress: 171/979 (17.5%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 643600, waiting 10 seconds...\n",
      "Progress: 181/979 (18.5%) - Rate: 0.5 req/sec\n",
      "Progress: 191/979 (19.5%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 668255, waiting 10 seconds...\n",
      "Rate limited on object 236138, waiting 10 seconds...\n",
      "Rate limited on object 236138, waiting 10 seconds...\n",
      "Rate limited on object 236138, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_200_20250705_140449.json\n",
      "Progress: 201/979 (20.5%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 72339, waiting 10 seconds...\n",
      "Progress: 211/979 (21.6%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 479915, waiting 10 seconds...\n",
      "Rate limited on object 846508, waiting 10 seconds...\n",
      "Rate limited on object 846509, waiting 10 seconds...\n",
      "Progress: 221/979 (22.6%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 479927, waiting 10 seconds...\n",
      "Progress: 231/979 (23.6%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 307929, waiting 10 seconds...\n",
      "Progress: 241/979 (24.6%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 477934, waiting 10 seconds...\n",
      "Rate limited on object 215793, waiting 10 seconds...\n",
      "Rate limited on object 219889, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_250_20250705_140643.json\n",
      "Progress: 251/979 (25.6%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 477942, waiting 10 seconds...\n",
      "Progress: 261/979 (26.7%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 224009, waiting 10 seconds...\n",
      "Progress: 271/979 (27.7%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 217935, waiting 10 seconds...\n",
      "Rate limited on object 217935, waiting 10 seconds...\n",
      "Rate limited on object 217935, waiting 10 seconds...\n",
      "Rate limited on object 226105, waiting 10 seconds...\n",
      "Progress: 281/979 (28.7%) - Rate: 0.4 req/sec\n",
      "Progress: 291/979 (29.7%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 213848, waiting 10 seconds...\n",
      "Rate limited on object 213853, waiting 10 seconds...\n",
      "Rate limited on object 213853, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_300_20250705_140838.json\n",
      "Progress: 301/979 (30.7%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 213859, waiting 10 seconds...\n",
      "Progress: 311/979 (31.8%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 443241, waiting 10 seconds...\n",
      "Progress: 321/979 (32.8%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 213873, waiting 10 seconds...\n",
      "Rate limited on object 480120, waiting 10 seconds...\n",
      "Rate limited on object 480120, waiting 10 seconds...\n",
      "Progress: 331/979 (33.8%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 445310, waiting 10 seconds...\n",
      "Rate limited on object 213886, waiting 10 seconds...\n",
      "Progress: 341/979 (34.8%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 217997, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_350_20250705_141021.json\n",
      "Progress: 351/979 (35.9%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 215956, waiting 10 seconds...\n",
      "Rate limited on object 215956, waiting 10 seconds...\n",
      "Rate limited on object 213920, waiting 10 seconds...\n",
      "Progress: 361/979 (36.9%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 215986, waiting 10 seconds...\n",
      "Progress: 371/979 (37.9%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 213961, waiting 10 seconds...\n",
      "Rate limited on object 213961, waiting 10 seconds...\n",
      "Rate limited on object 213961, waiting 10 seconds...\n",
      "Progress: 381/979 (38.9%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 213966, waiting 10 seconds...\n",
      "Progress: 391/979 (39.9%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 213993, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_400_20250705_141216.json\n",
      "Progress: 401/979 (41.0%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 213999, waiting 10 seconds...\n",
      "Rate limited on object 480242, waiting 10 seconds...\n",
      "Progress: 411/979 (42.0%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 214004, waiting 10 seconds...\n",
      "Rate limited on object 480249, waiting 10 seconds...\n",
      "Progress: 421/979 (43.0%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 214013, waiting 10 seconds...\n",
      "Progress: 431/979 (44.0%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 224272, waiting 10 seconds...\n",
      "Rate limited on object 224273, waiting 10 seconds...\n",
      "Rate limited on object 214043, waiting 10 seconds...\n",
      "Rate limited on object 224285, waiting 10 seconds...\n",
      "Progress: 441/979 (45.0%) - Rate: 0.4 req/sec\n",
      "Saved batch to remaining_textiles_batch_450_20250705_141410.json\n",
      "Progress: 451/979 (46.1%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 447529, waiting 10 seconds...\n",
      "Rate limited on object 224296, waiting 10 seconds...\n",
      "Rate limited on object 224301, waiting 10 seconds...\n",
      "Progress: 461/979 (47.1%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 224307, waiting 10 seconds...\n",
      "Rate limited on object 224311, waiting 10 seconds...\n",
      "Progress: 471/979 (48.1%) - Rate: 0.4 req/sec\n",
      "Progress: 481/979 (49.1%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 214096, waiting 10 seconds...\n",
      "Rate limited on object 214097, waiting 10 seconds...\n",
      "Progress: 491/979 (50.2%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 214101, waiting 10 seconds...\n",
      "Rate limited on object 789594, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_500_20250705_141603.json\n",
      "Progress: 501/979 (51.2%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 214109, waiting 10 seconds...\n",
      "Progress: 511/979 (52.2%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 214118, waiting 10 seconds...\n",
      "Rate limited on object 35948, waiting 10 seconds...\n",
      "Rate limited on object 320624, waiting 10 seconds...\n",
      "Progress: 521/979 (53.2%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 214124, waiting 10 seconds...\n",
      "Rate limited on object 224517, waiting 10 seconds...\n",
      "Progress: 531/979 (54.2%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 228501, waiting 10 seconds...\n",
      "Rate limited on object 736408, waiting 10 seconds...\n",
      "Progress: 541/979 (55.3%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 480424, waiting 10 seconds...\n",
      "Rate limited on object 722092, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_550_20250705_141808.json\n",
      "Progress: 551/979 (56.3%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 214202, waiting 10 seconds...\n",
      "Progress: 561/979 (57.3%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 214219, waiting 10 seconds...\n",
      "Rate limited on object 214219, waiting 10 seconds...\n",
      "Progress: 571/979 (58.3%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 722129, waiting 10 seconds...\n",
      "Rate limited on object 214228, waiting 10 seconds...\n",
      "Progress: 581/979 (59.3%) - Rate: 0.4 req/sec\n",
      "Progress: 591/979 (60.4%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 68853, waiting 10 seconds...\n",
      "Rate limited on object 447737, waiting 10 seconds...\n",
      "Rate limited on object 212223, waiting 10 seconds...\n",
      "Rate limited on object 212225, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_600_20250705_142001.json\n",
      "Progress: 601/979 (61.4%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 212232, waiting 10 seconds...\n",
      "Progress: 611/979 (62.4%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 212242, waiting 10 seconds...\n",
      "Progress: 621/979 (63.4%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 212245, waiting 10 seconds...\n",
      "Rate limited on object 212248, waiting 10 seconds...\n",
      "Rate limited on object 212250, waiting 10 seconds...\n",
      "Progress: 631/979 (64.5%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 224544, waiting 10 seconds...\n",
      "Progress: 641/979 (65.5%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 222510, waiting 10 seconds...\n",
      "Rate limited on object 212283, waiting 10 seconds...\n",
      "Rate limited on object 699716, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_650_20250705_142155.json\n",
      "Progress: 651/979 (66.5%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 443728, waiting 10 seconds...\n",
      "Rate limited on object 238936, waiting 10 seconds...\n",
      "Progress: 661/979 (67.5%) - Rate: 0.4 req/sec\n",
      "Progress: 671/979 (68.5%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 218517, waiting 10 seconds...\n",
      "Rate limited on object 746902, waiting 10 seconds...\n",
      "Progress: 681/979 (69.6%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 218531, waiting 10 seconds...\n",
      "Rate limited on object 216496, waiting 10 seconds...\n",
      "Progress: 691/979 (70.6%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 218561, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_700_20250705_142330.json\n",
      "Progress: 701/979 (71.6%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 230863, waiting 10 seconds...\n",
      "Rate limited on object 312785, waiting 10 seconds...\n",
      "Rate limited on object 228830, waiting 10 seconds...\n",
      "Progress: 711/979 (72.6%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 222700, waiting 10 seconds...\n",
      "Rate limited on object 214517, waiting 10 seconds...\n",
      "Progress: 721/979 (73.6%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 312825, waiting 10 seconds...\n",
      "Rate limited on object 312831, waiting 10 seconds...\n",
      "Progress: 731/979 (74.7%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 312837, waiting 10 seconds...\n",
      "Rate limited on object 67083, waiting 10 seconds...\n",
      "Progress: 741/979 (75.7%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 222746, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_750_20250705_142534.json\n",
      "Progress: 751/979 (76.7%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 312864, waiting 10 seconds...\n",
      "Rate limited on object 312866, waiting 10 seconds...\n",
      "Progress: 761/979 (77.7%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 312876, waiting 10 seconds...\n",
      "Rate limited on object 218686, waiting 10 seconds...\n",
      "Progress: 771/979 (78.8%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 218716, waiting 10 seconds...\n",
      "Progress: 781/979 (79.8%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 321123, waiting 10 seconds...\n",
      "Rate limited on object 214630, waiting 10 seconds...\n",
      "Progress: 791/979 (80.8%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 214637, waiting 10 seconds...\n",
      "Rate limited on object 214641, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_800_20250705_142728.json\n",
      "Progress: 801/979 (81.8%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 700028, waiting 10 seconds...\n",
      "Rate limited on object 321150, waiting 10 seconds...\n",
      "Progress: 811/979 (82.8%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 214668, waiting 10 seconds...\n",
      "Rate limited on object 757394, waiting 10 seconds...\n",
      "Progress: 821/979 (83.9%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 321189, waiting 10 seconds...\n",
      "Progress: 831/979 (84.9%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 321197, waiting 10 seconds...\n",
      "Rate limited on object 313006, waiting 10 seconds...\n",
      "Progress: 841/979 (85.9%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 321202, waiting 10 seconds...\n",
      "Rate limited on object 224949, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_850_20250705_142922.json\n",
      "Progress: 851/979 (86.9%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 444095, waiting 10 seconds...\n",
      "Progress: 861/979 (87.9%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 208589, waiting 10 seconds...\n",
      "Rate limited on object 208589, waiting 10 seconds...\n",
      "Progress: 871/979 (89.0%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 892643, waiting 10 seconds...\n",
      "Rate limited on object 681713, waiting 10 seconds...\n",
      "Progress: 881/979 (90.0%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 229116, waiting 10 seconds...\n",
      "Rate limited on object 321286, waiting 10 seconds...\n",
      "Progress: 891/979 (91.0%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 32529, waiting 10 seconds...\n",
      "Rate limited on object 214814, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_900_20250705_143117.json\n",
      "Progress: 901/979 (92.0%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 229151, waiting 10 seconds...\n",
      "Rate limited on object 739129, waiting 10 seconds...\n",
      "Progress: 911/979 (93.1%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 479040, waiting 10 seconds...\n",
      "Rate limited on object 448327, waiting 10 seconds...\n",
      "Progress: 921/979 (94.1%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 648245, waiting 10 seconds...\n",
      "Rate limited on object 229210, waiting 10 seconds...\n",
      "Progress: 931/979 (95.1%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 757596, waiting 10 seconds...\n",
      "Progress: 941/979 (96.1%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 479088, waiting 10 seconds...\n",
      "Rate limited on object 648247, waiting 10 seconds...\n",
      "Rate limited on object 755598, waiting 10 seconds...\n",
      "Saved batch to remaining_textiles_batch_950_20250705_143321.json\n",
      "Progress: 951/979 (97.1%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 225191, waiting 10 seconds...\n",
      "Progress: 961/979 (98.2%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 229334, waiting 10 seconds...\n",
      "Progress: 971/979 (99.2%) - Rate: 0.4 req/sec\n",
      "Rate limited on object 219111, waiting 10 seconds...\n",
      "Rate limited on object 313339, waiting 10 seconds...\n",
      "\n",
      "🎉 REMAINING DOWNLOAD COMPLETE!\n",
      "⏱️  Total time: 36.7 minutes\n",
      "✅ Successfully downloaded: 736\n",
      "❌ Not found (404): 243\n",
      "💥 Failed after retries: 0\n",
      "📊 Success rate: 75.2%\n",
      "📁 Final file: remaining_textiles_complete_20250705_135732.json\n",
      "📝 Log file: remaining_download_log_20250705_135732.txt\n",
      "📋 Summary saved: remaining_download_summary_20250705_135732.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Load the complete object ID list\n",
    "with open(\"textile_object_ids.json\", \"r\") as f:\n",
    "    all_object_ids = json.load(f)\n",
    "\n",
    "# Load both downloaded JSON files to get what we already have\n",
    "forward_file = 'met_textiles_batch_22800_20250705_134702.json'\n",
    "reverse_file = 'idun/met_textiles_batch_11988_20250705_134921.json'\n",
    "\n",
    "all_downloaded_ids = set()\n",
    "\n",
    "# Get already downloaded IDs\n",
    "try:\n",
    "    with open(forward_file, 'r', encoding='utf-8') as f:\n",
    "        forward_data = json.load(f)\n",
    "    for obj in forward_data:\n",
    "        if isinstance(obj, dict) and 'objectID' in obj:\n",
    "            all_downloaded_ids.add(obj['objectID'])\n",
    "    print(f\"Loaded {len(all_downloaded_ids)} IDs from forward file\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading forward file: {e}\")\n",
    "\n",
    "try:\n",
    "    with open(reverse_file, 'r', encoding='utf-8') as f:\n",
    "        reverse_data = json.load(f)\n",
    "    for obj in reverse_data:\n",
    "        if isinstance(obj, dict) and 'objectID' in obj:\n",
    "            all_downloaded_ids.add(obj['objectID'])\n",
    "    print(f\"Total unique IDs after reverse: {len(all_downloaded_ids)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error loading reverse file: {e}\")\n",
    "\n",
    "# Calculate remaining IDs\n",
    "all_object_ids_set = set(all_object_ids)\n",
    "remaining_ids = list(all_object_ids_set - all_downloaded_ids)\n",
    "\n",
    "print(f\"Found {len(remaining_ids)} remaining objects to download\")\n",
    "\n",
    "# Function to get object details with retry logic\n",
    "def get_object_details(object_id, max_retries=5):\n",
    "    detail_url = f\"https://collectionapi.metmuseum.org/public/collection/v1/objects/{object_id}\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(detail_url)\n",
    "            if response.status_code == 200:\n",
    "                return response.json(), \"success\"\n",
    "            elif response.status_code == 403:\n",
    "                print(f\"Rate limited on object {object_id}, waiting 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            elif response.status_code == 404:\n",
    "                return None, \"not_found\"\n",
    "            else:\n",
    "                print(f\"Failed to get object {object_id}: {response.status_code}\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting object {object_id}: {e}\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return None, \"failed_after_retries\"\n",
    "\n",
    "# Download remaining objects\n",
    "downloaded_objects = []\n",
    "failed_ids = []\n",
    "not_found_ids = []\n",
    "batch_size = 50\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file = f\"remaining_download_log_{timestamp}.txt\"\n",
    "\n",
    "print(f\"Starting download of {len(remaining_ids)} remaining objects...\")\n",
    "print(f\"Log file: {log_file}\")\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "with open(log_file, 'w') as log:\n",
    "    log.write(f\"Download started at {datetime.now()}\\n\")\n",
    "    log.write(f\"Remaining objects to download: {len(remaining_ids)}\\n\\n\")\n",
    "    \n",
    "    for i, object_id in enumerate(remaining_ids):\n",
    "        if i % 10 == 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            rate = i / elapsed if elapsed > 0 else 0\n",
    "            progress_msg = f\"Progress: {i+1}/{len(remaining_ids)} ({(i+1)/len(remaining_ids)*100:.1f}%) - Rate: {rate:.1f} req/sec\"\n",
    "            print(progress_msg)\n",
    "            log.write(f\"{progress_msg}\\n\")\n",
    "            log.flush()\n",
    "        \n",
    "        object_data, status = get_object_details(object_id)\n",
    "        \n",
    "        if status == \"success\" and object_data:\n",
    "            downloaded_objects.append(object_data)\n",
    "            log.write(f\"✅ {object_id}: Downloaded successfully\\n\")\n",
    "        elif status == \"not_found\":\n",
    "            not_found_ids.append(object_id)\n",
    "            log.write(f\"❌ {object_id}: Not found (404)\\n\")\n",
    "        else:\n",
    "            failed_ids.append(object_id)\n",
    "            log.write(f\"💥 {object_id}: Failed after retries\\n\")\n",
    "        \n",
    "        # Save progress every batch_size objects\n",
    "        if (i + 1) % batch_size == 0:\n",
    "            batch_timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            filename = f\"remaining_textiles_batch_{i+1}_{batch_timestamp}.json\"\n",
    "            with open(filename, 'w', encoding='utf-8') as f:\n",
    "                json.dump(downloaded_objects, f, indent=2, ensure_ascii=False)\n",
    "            log.write(f\"💾 Saved {len(downloaded_objects)} objects to {filename}\\n\")\n",
    "            print(f\"Saved batch to {filename}\")\n",
    "        \n",
    "        # Slower rate for remaining objects\n",
    "        time.sleep(0.2)  # 5 requests per second\n",
    "    \n",
    "    # Final summary\n",
    "    total_time = time.time() - start_time\n",
    "    log.write(f\"\\n=== FINAL SUMMARY ===\\n\")\n",
    "    log.write(f\"Total time: {total_time/60:.1f} minutes\\n\")\n",
    "    log.write(f\"Successfully downloaded: {len(downloaded_objects)}\\n\")\n",
    "    log.write(f\"Not found (404): {len(not_found_ids)}\\n\")\n",
    "    log.write(f\"Failed after retries: {len(failed_ids)}\\n\")\n",
    "    log.write(f\"Success rate: {len(downloaded_objects)/len(remaining_ids)*100:.1f}%\\n\")\n",
    "\n",
    "# Save final results\n",
    "final_filename = f\"remaining_textiles_complete_{timestamp}.json\"\n",
    "with open(final_filename, 'w', encoding='utf-8') as f:\n",
    "    json.dump(downloaded_objects, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "# Save failed and not found IDs\n",
    "with open(f\"not_found_ids_{timestamp}.json\", 'w') as f:\n",
    "    json.dump(not_found_ids, f)\n",
    "\n",
    "with open(f\"failed_ids_{timestamp}.json\", 'w') as f:\n",
    "    json.dump(failed_ids, f)\n",
    "\n",
    "# Final summary\n",
    "print(f\"\\n🎉 REMAINING DOWNLOAD COMPLETE!\")\n",
    "print(f\"⏱️  Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "print(f\"✅ Successfully downloaded: {len(downloaded_objects)}\")\n",
    "print(f\"❌ Not found (404): {len(not_found_ids)}\")\n",
    "print(f\"💥 Failed after retries: {len(failed_ids)}\")\n",
    "print(f\"📊 Success rate: {len(downloaded_objects)/len(remaining_ids)*100:.1f}%\")\n",
    "print(f\"📁 Final file: {final_filename}\")\n",
    "print(f\"📝 Log file: {log_file}\")\n",
    "\n",
    "# Create summary JSON\n",
    "summary = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"total_remaining\": len(remaining_ids),\n",
    "    \"successfully_downloaded\": len(downloaded_objects),\n",
    "    \"not_found_404\": len(not_found_ids),\n",
    "    \"failed_after_retries\": len(failed_ids),\n",
    "    \"success_rate_percent\": len(downloaded_objects)/len(remaining_ids)*100,\n",
    "    \"download_time_minutes\": (time.time() - start_time)/60,\n",
    "    \"files_created\": {\n",
    "        \"main_data\": final_filename,\n",
    "        \"log\": log_file,\n",
    "        \"not_found_ids\": f\"not_found_ids_{timestamp}.json\",\n",
    "        \"failed_ids\": f\"failed_ids_{timestamp}.json\"\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(f\"remaining_download_summary_{timestamp}.json\", 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(f\"📋 Summary saved: remaining_download_summary_{timestamp}.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2066d5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8b27de79",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a84e47f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== PROPER TEXTILE SEARCH COMPARISON ===\n",
      "Search started at: 2025-07-05 19:08:48.561694\n",
      "\n",
      "🔍 Searching: Textiles Only\n",
      "   Parameters: {'medium': 'Textiles', 'q': '*'}\n",
      "   ✅ Found: 33,437 objects\n",
      "\n",
      "🔍 Searching: Tapestries Only\n",
      "   Parameters: {'medium': 'Tapestries', 'q': '*'}\n",
      "   ✅ Found: 2,355 objects\n",
      "\n",
      "🔍 Searching: Textiles + Tapestries (Proper Way)\n",
      "   Parameters: {'medium': 'Textiles|Tapestries', 'q': '*'}\n",
      "   ✅ Found: 2,204 objects\n",
      "\n",
      "🔍 Searching: All Textile-related\n",
      "   Parameters: {'medium': 'Textiles|Tapestries|Embroidery|Lace', 'q': '*'}\n",
      "   ✅ Found: 0 objects\n",
      "\n",
      "=== RESULTS SUMMARY ===\n",
      "Textiles Only: 33,437 objects\n",
      "Tapestries Only: 2,355 objects\n",
      "Textiles + Tapestries (Proper Way): 2,204 objects\n",
      "All Textile-related: 0 objects\n",
      "\n",
      "=== DETAILED OVERLAP ANALYSIS ===\n",
      "📊 Textiles only: 31,233\n",
      "📊 Tapestries only: 151\n",
      "📊 Overlap (objects in both): 2,204\n",
      "📊 Theoretical union: 33,588\n",
      "📊 API combined search (Textiles|Tapestries): 2,204\n",
      "❓ Difference: 31,384 objects\n",
      "   Missing from API: 31,384 objects\n",
      "   Sample missing: [237, 13561, 13562, 13735, 13737]\n",
      "\n",
      "📋 Sample objects that are both Textiles AND Tapestries:\n",
      "    [37614, 39495, 39642, 39728, 39731, 39732, 39733, 39734, 39738, 42116]\n",
      "\n",
      "=== VERIFICATION ===\n",
      "Your original count: 33,437\n",
      "Current API count: 33,437\n",
      "✅ Counts match perfectly!\n",
      "\n",
      "📁 Detailed results saved to: proper_textile_search_analysis_20250705_190858.json\n",
      "✅ Analysis complete at: 2025-07-05 19:08:58.914495\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def search_with_retry(search_params, search_name, max_retries=5):\n",
    "    \"\"\"Search with retry logic and return count and object IDs\"\"\"\n",
    "    search_url = \"https://collectionapi.metmuseum.org/public/collection/v1/search\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(search_url, params=search_params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return data.get(\"total\", 0), data.get(\"objectIDs\", [])\n",
    "            elif response.status_code == 403:\n",
    "                print(f\"Rate limited for '{search_name}', waiting 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Error {response.status_code} for '{search_name}', retrying...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Exception for '{search_name}': {e}, retrying...\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    print(f\"Failed to get data for '{search_name}' after {max_retries} attempts\")\n",
    "    return 0, []\n",
    "\n",
    "# Define search configurations\n",
    "searches = [\n",
    "    {\n",
    "        \"name\": \"Textiles Only\",\n",
    "        \"params\": {\n",
    "            \"medium\": \"Textiles\",\n",
    "            \"q\": \"*\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Tapestries Only\", \n",
    "        \"params\": {\n",
    "            \"medium\": \"Tapestries\",\n",
    "            \"q\": \"*\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Textiles + Tapestries (Proper Way)\",\n",
    "        \"params\": {\n",
    "            \"medium\": \"Textiles|Tapestries\",  # Using | operator\n",
    "            \"q\": \"*\"\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"All Textile-related\",\n",
    "        \"params\": {\n",
    "            \"medium\": \"Textiles|Tapestries|Embroidery|Lace\",\n",
    "            \"q\": \"*\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"=== PROPER TEXTILE SEARCH COMPARISON ===\")\n",
    "print(f\"Search started at: {datetime.now()}\")\n",
    "print()\n",
    "\n",
    "results = {}\n",
    "\n",
    "for search_config in searches:\n",
    "    search_name = search_config[\"name\"]\n",
    "    search_params = search_config[\"params\"]\n",
    "    \n",
    "    print(f\"🔍 Searching: {search_name}\")\n",
    "    print(f\"   Parameters: {search_params}\")\n",
    "    \n",
    "    count, object_ids = search_with_retry(search_params, search_name)\n",
    "    results[search_name] = {\n",
    "        \"count\": count,\n",
    "        \"object_ids\": set(object_ids) if object_ids else set(),\n",
    "        \"params\": search_params\n",
    "    }\n",
    "    print(f\"   ✅ Found: {count:,} objects\")\n",
    "    print()\n",
    "    \n",
    "    # Small delay between searches\n",
    "    time.sleep(2)\n",
    "\n",
    "print(f\"=== RESULTS SUMMARY ===\")\n",
    "for search_name, data in results.items():\n",
    "    print(f\"{search_name}: {data['count']:,} objects\")\n",
    "\n",
    "# Calculate overlaps and differences\n",
    "textiles_ids = results.get(\"Textiles Only\", {}).get(\"object_ids\", set())\n",
    "tapestries_ids = results.get(\"Tapestries Only\", {}).get(\"object_ids\", set())\n",
    "combined_ids = results.get(\"Textiles + Tapestries (Proper Way)\", {}).get(\"object_ids\", set())\n",
    "\n",
    "print(f\"\\n=== DETAILED OVERLAP ANALYSIS ===\")\n",
    "\n",
    "if textiles_ids and tapestries_ids:\n",
    "    overlap = textiles_ids & tapestries_ids\n",
    "    textiles_only = textiles_ids - tapestries_ids\n",
    "    tapestries_only = tapestries_ids - textiles_ids\n",
    "    \n",
    "    print(f\"📊 Textiles only: {len(textiles_only):,}\")\n",
    "    print(f\"📊 Tapestries only: {len(tapestries_only):,}\")\n",
    "    print(f\"📊 Overlap (objects in both): {len(overlap):,}\")\n",
    "    \n",
    "    theoretical_union = textiles_ids | tapestries_ids\n",
    "    print(f\"📊 Theoretical union: {len(theoretical_union):,}\")\n",
    "    \n",
    "    if combined_ids:\n",
    "        print(f\"📊 API combined search (Textiles|Tapestries): {len(combined_ids):,}\")\n",
    "        \n",
    "        # Check if API combined search matches theoretical union\n",
    "        if len(combined_ids) == len(theoretical_union):\n",
    "            print(\"✅ API combined search matches theoretical union perfectly!\")\n",
    "        else:\n",
    "            difference = len(theoretical_union) - len(combined_ids)\n",
    "            print(f\"❓ Difference: {difference:,} objects\")\n",
    "            \n",
    "            # Check which objects are missing/extra\n",
    "            missing_from_api = theoretical_union - combined_ids\n",
    "            extra_in_api = combined_ids - theoretical_union\n",
    "            \n",
    "            if missing_from_api:\n",
    "                print(f\"   Missing from API: {len(missing_from_api):,} objects\")\n",
    "                print(f\"   Sample missing: {sorted(list(missing_from_api))[:5]}\")\n",
    "            \n",
    "            if extra_in_api:\n",
    "                print(f\"   Extra in API: {len(extra_in_api):,} objects\")\n",
    "                print(f\"   Sample extra: {sorted(list(extra_in_api))[:5]}\")\n",
    "    \n",
    "    # Show sample overlapping objects\n",
    "    if len(overlap) > 0:\n",
    "        print(f\"\\n📋 Sample objects that are both Textiles AND Tapestries:\")\n",
    "        print(f\"    {sorted(list(overlap))[:10]}\")\n",
    "\n",
    "# Verify your original textile count\n",
    "original_count = 33437\n",
    "textiles_count = results.get(\"Textiles Only\", {}).get(\"count\", 0)\n",
    "print(f\"\\n=== VERIFICATION ===\")\n",
    "print(f\"Your original count: {original_count:,}\")\n",
    "print(f\"Current API count: {textiles_count:,}\")\n",
    "\n",
    "if textiles_count == original_count:\n",
    "    print(\"✅ Counts match perfectly!\")\n",
    "elif abs(textiles_count - original_count) < 100:\n",
    "    print(f\"⚠️  Small difference: {abs(textiles_count - original_count)} objects\")\n",
    "else:\n",
    "    print(f\"❌ Significant difference: {abs(textiles_count - original_count)} objects\")\n",
    "\n",
    "# Save detailed results\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "detailed_results = {\n",
    "    \"timestamp\": timestamp,\n",
    "    \"search_results\": {\n",
    "        name: {\n",
    "            \"count\": data[\"count\"],\n",
    "            \"parameters\": data[\"params\"]\n",
    "        } for name, data in results.items()\n",
    "    },\n",
    "    \"overlap_analysis\": {\n",
    "        \"textiles_only\": len(textiles_only) if textiles_ids and tapestries_ids else 0,\n",
    "        \"tapestries_only\": len(tapestries_only) if textiles_ids and tapestries_ids else 0,\n",
    "        \"overlap\": len(overlap) if textiles_ids and tapestries_ids else 0,\n",
    "        \"theoretical_union\": len(theoretical_union) if textiles_ids and tapestries_ids else 0,\n",
    "        \"api_combined\": len(combined_ids) if combined_ids else 0\n",
    "    },\n",
    "    \"verification\": {\n",
    "        \"original_textile_count\": original_count,\n",
    "        \"current_api_count\": textiles_count,\n",
    "        \"difference\": abs(textiles_count - original_count)\n",
    "    }\n",
    "}\n",
    "\n",
    "filename = f\"proper_textile_search_analysis_{timestamp}.json\"\n",
    "with open(filename, \"w\") as f:\n",
    "    json.dump(detailed_results, f, indent=2)\n",
    "\n",
    "print(f\"\\n📁 Detailed results saved to: {filename}\")\n",
    "print(f\"✅ Analysis complete at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dad99780",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINDING MISSING TAPESTRY OBJECTS ===\n",
      "Started at: 2025-07-05 19:19:09.003753\n",
      "\n",
      "🔍 Step 1: Getting Textile and Tapestry object IDs...\n",
      "✅ Textiles: 33,437 objects\n",
      "✅ Tapestries: 2,355 objects\n",
      "\n",
      "📊 Step 2: Analyzing overlap...\n",
      "📋 Total tapestries: 2,355\n",
      "📋 Tapestries also in textiles: 2,204\n",
      "🎯 Tapestries ONLY (missing from textiles): 151\n",
      "\n",
      "💾 Saved 151 missing tapestry IDs to: missing_tapestry_ids_20250705_191910.json\n",
      "📋 Sample missing IDs: [237, 14147, 14148, 14149, 14150, 14151, 14152, 14153, 14154, 14155]\n",
      "\n",
      "🚀 Step 3: Downloading details for 151 missing tapestry objects...\n",
      "Progress: 1/151 (0.7%) - Rate: 0.0 req/sec\n",
      "Progress: 11/151 (7.3%) - Rate: 2.2 req/sec\n",
      "Progress: 21/151 (13.9%) - Rate: 2.2 req/sec\n",
      "Rate limited on object 51485, waiting 10 seconds...\n",
      "Rate limited on object 51485, waiting 10 seconds...\n",
      "Rate limited on object 51485, waiting 10 seconds...\n",
      "Rate limited on object 51485, waiting 10 seconds...\n",
      "Progress: 31/151 (20.5%) - Rate: 0.6 req/sec\n",
      "Rate limited on object 53714, waiting 10 seconds...\n",
      "Progress: 41/151 (27.2%) - Rate: 0.6 req/sec\n",
      "💾 Saved batch to missing_tapestries_batch_50_20250705_191910.json\n",
      "Progress: 51/151 (33.8%) - Rate: 0.7 req/sec\n",
      "Rate limited on object 69159, waiting 10 seconds...\n",
      "Rate limited on object 69159, waiting 10 seconds...\n",
      "Rate limited on object 69159, waiting 10 seconds...\n",
      "Rate limited on object 69159, waiting 10 seconds...\n",
      "Rate limited on object 69452, waiting 10 seconds...\n",
      "Progress: 61/151 (40.4%) - Rate: 0.5 req/sec\n",
      "Progress: 71/151 (47.0%) - Rate: 0.5 req/sec\n",
      "Progress: 81/151 (53.6%) - Rate: 0.6 req/sec\n",
      "Rate limited on object 70356, waiting 10 seconds...\n",
      "Rate limited on object 70357, waiting 10 seconds...\n",
      "Rate limited on object 70357, waiting 10 seconds...\n",
      "Rate limited on object 70537, waiting 10 seconds...\n",
      "Progress: 91/151 (60.3%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 86274, waiting 10 seconds...\n",
      "💾 Saved batch to missing_tapestries_batch_100_20250705_191910.json\n",
      "Progress: 101/151 (66.9%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 189337, waiting 10 seconds...\n",
      "Rate limited on object 189343, waiting 10 seconds...\n",
      "Rate limited on object 189343, waiting 10 seconds...\n",
      "Progress: 111/151 (73.5%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 192755, waiting 10 seconds...\n",
      "Progress: 121/151 (80.1%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 207558, waiting 10 seconds...\n",
      "Progress: 131/151 (86.8%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 232179, waiting 10 seconds...\n",
      "Rate limited on object 232180, waiting 10 seconds...\n",
      "Rate limited on object 232180, waiting 10 seconds...\n",
      "Rate limited on object 238006, waiting 10 seconds...\n",
      "Progress: 141/151 (93.4%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 239170, waiting 10 seconds...\n",
      "💾 Saved batch to missing_tapestries_batch_150_20250705_191910.json\n",
      "Progress: 151/151 (100.0%) - Rate: 0.5 req/sec\n",
      "\n",
      "🎉 MISSING TAPESTRIES DOWNLOAD COMPLETE!\n",
      "⏱️  Total time: 5.4 minutes\n",
      "✅ Successfully downloaded: 150\n",
      "🖼️  Objects with images: 102\n",
      "❌ Not found (404): 1\n",
      "💥 Failed after retries: 0\n",
      "📊 Success rate: 99.3%\n",
      "📁 Main file: missing_tapestries_complete_20250705_191910.json\n",
      "📝 Log file: missing_tapestries_download_log_20250705_191910.txt\n",
      "📋 Summary saved: missing_tapestries_summary_20250705_191910.json\n",
      "\n",
      "✅ Analysis complete at: 2025-07-05 19:24:34.597267\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import time\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "def search_with_retry(search_params, search_name, max_retries=5):\n",
    "    \"\"\"Search with retry logic and return count and object IDs\"\"\"\n",
    "    search_url = \"https://collectionapi.metmuseum.org/public/collection/v1/search\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(search_url, params=search_params)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                return data.get(\"total\", 0), data.get(\"objectIDs\", [])\n",
    "            elif response.status_code == 403:\n",
    "                print(f\"Rate limited for '{search_name}', waiting 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            else:\n",
    "                print(f\"Error {response.status_code} for '{search_name}', retrying...\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Exception for '{search_name}': {e}, retrying...\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    print(f\"Failed to get data for '{search_name}' after {max_retries} attempts\")\n",
    "    return 0, []\n",
    "\n",
    "def get_object_details(object_id, max_retries=5):\n",
    "    \"\"\"Get object details with retry logic\"\"\"\n",
    "    detail_url = f\"https://collectionapi.metmuseum.org/public/collection/v1/objects/{object_id}\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(detail_url)\n",
    "            if response.status_code == 200:\n",
    "                return response.json(), \"success\"\n",
    "            elif response.status_code == 403:\n",
    "                print(f\"Rate limited on object {object_id}, waiting 10 seconds...\")\n",
    "                time.sleep(10)\n",
    "                continue\n",
    "            elif response.status_code == 404:\n",
    "                return None, \"not_found\"\n",
    "            else:\n",
    "                print(f\"Failed to get object {object_id}: {response.status_code}\")\n",
    "                time.sleep(2)\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting object {object_id}: {e}\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    return None, \"failed_after_retries\"\n",
    "\n",
    "print(\"=== FINDING MISSING TAPESTRY OBJECTS ===\")\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "\n",
    "# Step 1: Get all textile and tapestry object IDs\n",
    "print(\"\\n🔍 Step 1: Getting Textile and Tapestry object IDs...\")\n",
    "\n",
    "textiles_count, textiles_ids = search_with_retry(\n",
    "    {\"medium\": \"Textiles\", \"q\": \"*\"}, \n",
    "    \"Textiles Only\"\n",
    ")\n",
    "print(f\"✅ Textiles: {textiles_count:,} objects\")\n",
    "\n",
    "tapestries_count, tapestries_ids = search_with_retry(\n",
    "    {\"medium\": \"Tapestries\", \"q\": \"*\"}, \n",
    "    \"Tapestries Only\"\n",
    ")\n",
    "print(f\"✅ Tapestries: {tapestries_count:,} objects\")\n",
    "\n",
    "# Convert to sets for easier manipulation\n",
    "textiles_set = set(textiles_ids) if textiles_ids else set()\n",
    "tapestries_set = set(tapestries_ids) if tapestries_ids else set()\n",
    "\n",
    "# Step 2: Find tapestries that are NOT in textiles\n",
    "print(\"\\n📊 Step 2: Analyzing overlap...\")\n",
    "tapestries_only = tapestries_set - textiles_set  # Tapestries NOT in textiles\n",
    "overlap = textiles_set & tapestries_set  # Objects in both\n",
    "\n",
    "print(f\"📋 Total tapestries: {len(tapestries_set):,}\")\n",
    "print(f\"📋 Tapestries also in textiles: {len(overlap):,}\")\n",
    "print(f\"🎯 Tapestries ONLY (missing from textiles): {len(tapestries_only):,}\")\n",
    "\n",
    "# Step 3: Save the missing tapestry IDs\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "missing_ids_list = sorted(list(tapestries_only))\n",
    "\n",
    "# Save missing IDs to file\n",
    "missing_ids_file = f\"missing_tapestry_ids_{timestamp}.json\"\n",
    "with open(missing_ids_file, 'w') as f:\n",
    "    json.dump(missing_ids_list, f, indent=2)\n",
    "\n",
    "print(f\"\\n💾 Saved {len(missing_ids_list)} missing tapestry IDs to: {missing_ids_file}\")\n",
    "print(f\"📋 Sample missing IDs: {missing_ids_list[:10]}\")\n",
    "\n",
    "# Step 4: Download details for missing tapestry objects\n",
    "if len(missing_ids_list) > 0:\n",
    "    print(f\"\\n🚀 Step 3: Downloading details for {len(missing_ids_list)} missing tapestry objects...\")\n",
    "    \n",
    "    downloaded_objects = []\n",
    "    failed_ids = []\n",
    "    not_found_ids = []\n",
    "    batch_size = 50\n",
    "    \n",
    "    log_file = f\"missing_tapestries_download_log_{timestamp}.txt\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with open(log_file, 'w') as log:\n",
    "        log.write(f\"Missing tapestries download started at {datetime.now()}\\n\")\n",
    "        log.write(f\"Objects to download: {len(missing_ids_list)}\\n\\n\")\n",
    "        \n",
    "        for i, object_id in enumerate(missing_ids_list):\n",
    "            if i % 10 == 0:\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = i / elapsed if elapsed > 0 else 0\n",
    "                progress_msg = f\"Progress: {i+1}/{len(missing_ids_list)} ({(i+1)/len(missing_ids_list)*100:.1f}%) - Rate: {rate:.1f} req/sec\"\n",
    "                print(progress_msg)\n",
    "                log.write(f\"{progress_msg}\\n\")\n",
    "                log.flush()\n",
    "            \n",
    "            object_data, status = get_object_details(object_id)\n",
    "            \n",
    "            if status == \"success\" and object_data:\n",
    "                downloaded_objects.append(object_data)\n",
    "                log.write(f\"✅ {object_id}: Downloaded successfully\\n\")\n",
    "            elif status == \"not_found\":\n",
    "                not_found_ids.append(object_id)\n",
    "                log.write(f\"❌ {object_id}: Not found (404)\\n\")\n",
    "            else:\n",
    "                failed_ids.append(object_id)\n",
    "                log.write(f\"💥 {object_id}: Failed after retries\\n\")\n",
    "            \n",
    "            # Save progress every batch_size objects\n",
    "            if (i + 1) % batch_size == 0:\n",
    "                batch_filename = f\"missing_tapestries_batch_{i+1}_{timestamp}.json\"\n",
    "                with open(batch_filename, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(downloaded_objects, f, indent=2, ensure_ascii=False)\n",
    "                log.write(f\"💾 Saved {len(downloaded_objects)} objects to {batch_filename}\\n\")\n",
    "                print(f\"💾 Saved batch to {batch_filename}\")\n",
    "            \n",
    "            # Reasonable rate limiting\n",
    "            time.sleep(0.2)  # 5 requests per second\n",
    "        \n",
    "        # Final summary in log\n",
    "        total_time = time.time() - start_time\n",
    "        log.write(f\"\\n=== DOWNLOAD SUMMARY ===\\n\")\n",
    "        log.write(f\"Total time: {total_time/60:.1f} minutes\\n\")\n",
    "        log.write(f\"Successfully downloaded: {len(downloaded_objects)}\\n\")\n",
    "        log.write(f\"Not found (404): {len(not_found_ids)}\\n\")\n",
    "        log.write(f\"Failed after retries: {len(failed_ids)}\\n\")\n",
    "        log.write(f\"Success rate: {len(downloaded_objects)/len(missing_ids_list)*100:.1f}%\\n\")\n",
    "    \n",
    "    # Save final results\n",
    "    final_filename = f\"missing_tapestries_complete_{timestamp}.json\"\n",
    "    with open(final_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(downloaded_objects, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    # Save failed and not found IDs\n",
    "    if not_found_ids:\n",
    "        with open(f\"missing_tapestries_not_found_{timestamp}.json\", 'w') as f:\n",
    "            json.dump(not_found_ids, f)\n",
    "    \n",
    "    if failed_ids:\n",
    "        with open(f\"missing_tapestries_failed_{timestamp}.json\", 'w') as f:\n",
    "            json.dump(failed_ids, f)\n",
    "    \n",
    "    # Count objects with images\n",
    "    objects_with_images = sum(1 for obj in downloaded_objects if obj.get('primaryImage'))\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n🎉 MISSING TAPESTRIES DOWNLOAD COMPLETE!\")\n",
    "    print(f\"⏱️  Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    print(f\"✅ Successfully downloaded: {len(downloaded_objects)}\")\n",
    "    print(f\"🖼️  Objects with images: {objects_with_images}\")\n",
    "    print(f\"❌ Not found (404): {len(not_found_ids)}\")\n",
    "    print(f\"💥 Failed after retries: {len(failed_ids)}\")\n",
    "    print(f\"📊 Success rate: {len(downloaded_objects)/len(missing_ids_list)*100:.1f}%\")\n",
    "    print(f\"📁 Main file: {final_filename}\")\n",
    "    print(f\"📝 Log file: {log_file}\")\n",
    "    \n",
    "    # Create comprehensive summary\n",
    "    summary = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"analysis\": {\n",
    "            \"total_textiles\": len(textiles_set),\n",
    "            \"total_tapestries\": len(tapestries_set),\n",
    "            \"tapestries_in_textiles\": len(overlap),\n",
    "            \"tapestries_missing_from_textiles\": len(tapestries_only)\n",
    "        },\n",
    "        \"download_results\": {\n",
    "            \"total_missing\": len(missing_ids_list),\n",
    "            \"successfully_downloaded\": len(downloaded_objects),\n",
    "            \"objects_with_images\": objects_with_images,\n",
    "            \"not_found_404\": len(not_found_ids),\n",
    "            \"failed_after_retries\": len(failed_ids),\n",
    "            \"success_rate_percent\": len(downloaded_objects)/len(missing_ids_list)*100 if missing_ids_list else 0,\n",
    "            \"download_time_minutes\": (time.time() - start_time)/60\n",
    "        },\n",
    "        \"files_created\": {\n",
    "            \"missing_ids\": missing_ids_file,\n",
    "            \"main_data\": final_filename,\n",
    "            \"log\": log_file,\n",
    "            \"not_found_ids\": f\"missing_tapestries_not_found_{timestamp}.json\" if not_found_ids else None,\n",
    "            \"failed_ids\": f\"missing_tapestries_failed_{timestamp}.json\" if failed_ids else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_file = f\"missing_tapestries_summary_{timestamp}.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    \n",
    "    print(f\"📋 Summary saved: {summary_file}\")\n",
    "\n",
    "else:\n",
    "    print(f\"\\n✅ No missing tapestry objects found - all tapestries are already included in textiles!\")\n",
    "\n",
    "print(f\"\\n✅ Analysis complete at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee0ae2c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RETRY FAILED DOWNLOADS ===\n",
      "Started at: 2025-07-05 22:06:44.719669\n",
      "📁 Loaded 243 IDs from not_found_ids_20250705_135732.json\n",
      "\n",
      "🎯 Total unique IDs to retry: 243\n",
      "📋 Sample retry IDs: [21107, 72339, 77239, 77240, 77241, 77242, 212220, 212221, 212223, 212224]\n",
      "\n",
      "🚀 Starting retry download...\n",
      "📝 Log file: retry_download_log_20250705_220644.txt\n",
      "Progress: 1/243 (0.4%) - Rate: 0.0 req/sec\n",
      "Progress: 6/243 (2.5%) - Rate: 1.7 req/sec\n",
      "Progress: 11/243 (4.5%) - Rate: 1.7 req/sec\n",
      "Progress: 16/243 (6.6%) - Rate: 1.7 req/sec\n",
      "Progress: 21/243 (8.6%) - Rate: 1.7 req/sec\n",
      "Progress: 26/243 (10.7%) - Rate: 1.7 req/sec\n",
      "Progress: 31/243 (12.8%) - Rate: 1.7 req/sec\n",
      "Rate limited on object 212246, waiting 15 seconds...\n",
      "Rate limited on object 212246, waiting 15 seconds...\n",
      "Rate limited on object 212246, waiting 15 seconds...\n",
      "Progress: 36/243 (14.8%) - Rate: 0.5 req/sec\n",
      "Progress: 41/243 (16.9%) - Rate: 0.6 req/sec\n",
      "Progress: 46/243 (18.9%) - Rate: 0.6 req/sec\n",
      "Progress: 51/243 (21.0%) - Rate: 0.7 req/sec\n",
      "Progress: 56/243 (23.0%) - Rate: 0.7 req/sec\n",
      "Rate limited on object 239288, waiting 15 seconds...\n",
      "Rate limited on object 239288, waiting 15 seconds...\n",
      "Rate limited on object 239288, waiting 15 seconds...\n",
      "Progress: 61/243 (25.1%) - Rate: 0.5 req/sec\n",
      "Progress: 66/243 (27.2%) - Rate: 0.5 req/sec\n",
      "Progress: 71/243 (29.2%) - Rate: 0.5 req/sec\n",
      "Progress: 76/243 (31.3%) - Rate: 0.6 req/sec\n",
      "Progress: 81/243 (33.3%) - Rate: 0.6 req/sec\n",
      "Progress: 86/243 (35.4%) - Rate: 0.6 req/sec\n",
      "Rate limited on object 321203, waiting 15 seconds...\n",
      "Rate limited on object 321203, waiting 15 seconds...\n",
      "Rate limited on object 321203, waiting 15 seconds...\n",
      "Progress: 91/243 (37.4%) - Rate: 0.5 req/sec\n",
      "Progress: 96/243 (39.5%) - Rate: 0.5 req/sec\n",
      "Progress: 101/243 (41.6%) - Rate: 0.5 req/sec\n",
      "Progress: 106/243 (43.6%) - Rate: 0.5 req/sec\n",
      "Progress: 111/243 (45.7%) - Rate: 0.6 req/sec\n",
      "Rate limited on object 477949, waiting 15 seconds...\n",
      "Rate limited on object 477949, waiting 15 seconds...\n",
      "Rate limited on object 477949, waiting 15 seconds...\n",
      "Progress: 116/243 (47.7%) - Rate: 0.5 req/sec\n",
      "Progress: 121/243 (49.8%) - Rate: 0.5 req/sec\n",
      "Progress: 126/243 (51.9%) - Rate: 0.5 req/sec\n",
      "Progress: 131/243 (53.9%) - Rate: 0.5 req/sec\n",
      "Progress: 136/243 (56.0%) - Rate: 0.5 req/sec\n",
      "Progress: 141/243 (58.0%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 642585, waiting 15 seconds...\n",
      "Rate limited on object 642585, waiting 15 seconds...\n",
      "Rate limited on object 642585, waiting 15 seconds...\n",
      "Progress: 146/243 (60.1%) - Rate: 0.5 req/sec\n",
      "Progress: 151/243 (62.1%) - Rate: 0.5 req/sec\n",
      "Progress: 156/243 (64.2%) - Rate: 0.5 req/sec\n",
      "Progress: 161/243 (66.3%) - Rate: 0.5 req/sec\n",
      "Progress: 166/243 (68.3%) - Rate: 0.5 req/sec\n",
      "Progress: 171/243 (70.4%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 708091, waiting 15 seconds...\n",
      "Rate limited on object 708091, waiting 15 seconds...\n",
      "Rate limited on object 708091, waiting 15 seconds...\n",
      "Progress: 176/243 (72.4%) - Rate: 0.5 req/sec\n",
      "Progress: 181/243 (74.5%) - Rate: 0.5 req/sec\n",
      "Progress: 186/243 (76.5%) - Rate: 0.5 req/sec\n",
      "Progress: 191/243 (78.6%) - Rate: 0.5 req/sec\n",
      "Progress: 196/243 (80.7%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 748569, waiting 15 seconds...\n",
      "Rate limited on object 748569, waiting 15 seconds...\n",
      "Rate limited on object 748569, waiting 15 seconds...\n",
      "Progress: 201/243 (82.7%) - Rate: 0.5 req/sec\n",
      "Progress: 206/243 (84.8%) - Rate: 0.5 req/sec\n",
      "Progress: 211/243 (86.8%) - Rate: 0.5 req/sec\n",
      "Progress: 216/243 (88.9%) - Rate: 0.5 req/sec\n",
      "Progress: 221/243 (90.9%) - Rate: 0.5 req/sec\n",
      "Progress: 226/243 (93.0%) - Rate: 0.5 req/sec\n",
      "Rate limited on object 845841, waiting 15 seconds...\n",
      "Rate limited on object 845841, waiting 15 seconds...\n",
      "Rate limited on object 845841, waiting 15 seconds...\n",
      "Progress: 231/243 (95.1%) - Rate: 0.5 req/sec\n",
      "Progress: 236/243 (97.1%) - Rate: 0.5 req/sec\n",
      "Progress: 241/243 (99.2%) - Rate: 0.5 req/sec\n",
      "\n",
      "🎉 RETRY DOWNLOAD COMPLETE!\n",
      "⏱️  Total time: 8.4 minutes\n",
      "✅ Successfully recovered: 0\n",
      "🖼️  Recovered objects with images: 0\n",
      "❌ Still not found (404): 243\n",
      "💥 Still failed: 0\n",
      "📊 Recovery rate: 0.0%\n",
      "📝 Log file: retry_download_log_20250705_220644.txt\n",
      "📋 Retry summary saved: retry_summary_20250705_220644.json\n",
      "\n",
      "✅ Retry process complete at: 2025-07-05 22:15:08.859526\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def get_object_details(object_id, max_retries=5):\n",
    "    \"\"\"Get object details with retry logic\"\"\"\n",
    "    detail_url = f\"https://collectionapi.metmuseum.org/public/collection/v1/objects/{object_id}\"\n",
    "    \n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            response = requests.get(detail_url)\n",
    "            if response.status_code == 200:\n",
    "                return response.json(), \"success\"\n",
    "            elif response.status_code == 403:\n",
    "                print(f\"Rate limited on object {object_id}, waiting 15 seconds...\")\n",
    "                time.sleep(15)  # Longer wait for rate limits\n",
    "                continue\n",
    "            elif response.status_code == 404:\n",
    "                return None, \"not_found\"\n",
    "            else:\n",
    "                print(f\"Failed to get object {object_id}: {response.status_code}\")\n",
    "                time.sleep(3)  # Slightly longer delay\n",
    "                continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting object {object_id}: {e}\")\n",
    "            time.sleep(3)\n",
    "    \n",
    "    return None, \"failed_after_retries\"\n",
    "\n",
    "print(\"=== RETRY FAILED DOWNLOADS ===\")\n",
    "print(f\"Started at: {datetime.now()}\")\n",
    "\n",
    "# Look for failed IDs files from previous runs\n",
    "failed_files = [\n",
    "    \"failed_ids_20250705_135732.json\",  # From your latest run\n",
    "    \"not_found_ids_20250705_135732.json\"  # Let's also retry the 404s\n",
    "]\n",
    "\n",
    "all_retry_ids = []\n",
    "\n",
    "# Load failed IDs from all available files\n",
    "for file in failed_files:\n",
    "    try:\n",
    "        with open(file, 'r') as f:\n",
    "            ids = json.load(f)\n",
    "            if ids:  # Only add if file contains IDs\n",
    "                all_retry_ids.extend(ids)\n",
    "                print(f\"📁 Loaded {len(ids)} IDs from {file}\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"❌ File not found: {file}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading {file}: {e}\")\n",
    "\n",
    "# Remove duplicates and sort\n",
    "retry_ids = sorted(list(set(all_retry_ids)))\n",
    "print(f\"\\n🎯 Total unique IDs to retry: {len(retry_ids)}\")\n",
    "\n",
    "if len(retry_ids) == 0:\n",
    "    print(\"✅ No failed IDs found to retry!\")\n",
    "else:\n",
    "    print(f\"📋 Sample retry IDs: {retry_ids[:10]}\")\n",
    "    \n",
    "    # Start retry process\n",
    "    downloaded_objects = []\n",
    "    still_failed_ids = []\n",
    "    still_not_found_ids = []\n",
    "    batch_size = 25  # Smaller batches for retry\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    log_file = f\"retry_download_log_{timestamp}.txt\"\n",
    "    \n",
    "    print(f\"\\n🚀 Starting retry download...\")\n",
    "    print(f\"📝 Log file: {log_file}\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    with open(log_file, 'w') as log:\n",
    "        log.write(f\"Retry download started at {datetime.now()}\\n\")\n",
    "        log.write(f\"Objects to retry: {len(retry_ids)}\\n\\n\")\n",
    "        \n",
    "        for i, object_id in enumerate(retry_ids):\n",
    "            if i % 5 == 0:  # More frequent progress updates\n",
    "                elapsed = time.time() - start_time\n",
    "                rate = i / elapsed if elapsed > 0 else 0\n",
    "                progress_msg = f\"Progress: {i+1}/{len(retry_ids)} ({(i+1)/len(retry_ids)*100:.1f}%) - Rate: {rate:.1f} req/sec\"\n",
    "                print(progress_msg)\n",
    "                log.write(f\"{progress_msg}\\n\")\n",
    "                log.flush()\n",
    "            \n",
    "            object_data, status = get_object_details(object_id, max_retries=7)  # More retries\n",
    "            \n",
    "            if status == \"success\" and object_data:\n",
    "                downloaded_objects.append(object_data)\n",
    "                log.write(f\"✅ {object_id}: Downloaded successfully on retry!\\n\")\n",
    "                print(f\"✅ Recovered object {object_id}\")\n",
    "            elif status == \"not_found\":\n",
    "                still_not_found_ids.append(object_id)\n",
    "                log.write(f\"❌ {object_id}: Still not found (404)\\n\")\n",
    "            else:\n",
    "                still_failed_ids.append(object_id)\n",
    "                log.write(f\"💥 {object_id}: Still failed after retries\\n\")\n",
    "            \n",
    "            # Save progress every batch_size objects\n",
    "            if (i + 1) % batch_size == 0 and downloaded_objects:\n",
    "                batch_filename = f\"retry_batch_{i+1}_{timestamp}.json\"\n",
    "                with open(batch_filename, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(downloaded_objects, f, indent=2, ensure_ascii=False)\n",
    "                log.write(f\"💾 Saved {len(downloaded_objects)} recovered objects to {batch_filename}\\n\")\n",
    "                print(f\"💾 Saved batch to {batch_filename}\")\n",
    "            \n",
    "            # Conservative rate limiting for retries\n",
    "            time.sleep(0.3)  # 3.3 requests per second\n",
    "        \n",
    "        # Final summary in log\n",
    "        total_time = time.time() - start_time\n",
    "        log.write(f\"\\n=== RETRY SUMMARY ===\\n\")\n",
    "        log.write(f\"Total time: {total_time/60:.1f} minutes\\n\")\n",
    "        log.write(f\"Successfully recovered: {len(downloaded_objects)}\\n\")\n",
    "        log.write(f\"Still not found (404): {len(still_not_found_ids)}\\n\")\n",
    "        log.write(f\"Still failed: {len(still_failed_ids)}\\n\")\n",
    "        log.write(f\"Recovery rate: {len(downloaded_objects)/len(retry_ids)*100:.1f}%\\n\")\n",
    "    \n",
    "    # Save final results\n",
    "    if downloaded_objects:\n",
    "        final_filename = f\"retry_recovered_objects_{timestamp}.json\"\n",
    "        with open(final_filename, 'w', encoding='utf-8') as f:\n",
    "            json.dump(downloaded_objects, f, indent=2, ensure_ascii=False)\n",
    "        print(f\"📁 Recovered objects saved to: {final_filename}\")\n",
    "    \n",
    "    # Save still failed IDs\n",
    "    if still_failed_ids:\n",
    "        with open(f\"still_failed_ids_{timestamp}.json\", 'w') as f:\n",
    "            json.dump(still_failed_ids, f)\n",
    "    \n",
    "    if still_not_found_ids:\n",
    "        with open(f\"still_not_found_ids_{timestamp}.json\", 'w') as f:\n",
    "            json.dump(still_not_found_ids, f)\n",
    "    \n",
    "    # Count objects with images\n",
    "    objects_with_images = sum(1 for obj in downloaded_objects if obj.get('primaryImage'))\n",
    "    \n",
    "    # Final summary\n",
    "    print(f\"\\n🎉 RETRY DOWNLOAD COMPLETE!\")\n",
    "    print(f\"⏱️  Total time: {(time.time() - start_time)/60:.1f} minutes\")\n",
    "    print(f\"✅ Successfully recovered: {len(downloaded_objects)}\")\n",
    "    print(f\"🖼️  Recovered objects with images: {objects_with_images}\")\n",
    "    print(f\"❌ Still not found (404): {len(still_not_found_ids)}\")\n",
    "    print(f\"💥 Still failed: {len(still_failed_ids)}\")\n",
    "    print(f\"📊 Recovery rate: {len(downloaded_objects)/len(retry_ids)*100:.1f}%\")\n",
    "    \n",
    "    if downloaded_objects:\n",
    "        print(f\"📁 Recovered objects file: {final_filename}\")\n",
    "    \n",
    "    print(f\"📝 Log file: {log_file}\")\n",
    "    \n",
    "    # Create retry summary\n",
    "    retry_summary = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"retry_results\": {\n",
    "            \"total_retry_attempts\": len(retry_ids),\n",
    "            \"successfully_recovered\": len(downloaded_objects),\n",
    "            \"objects_with_images\": objects_with_images,\n",
    "            \"still_not_found_404\": len(still_not_found_ids),\n",
    "            \"still_failed\": len(still_failed_ids),\n",
    "            \"recovery_rate_percent\": len(downloaded_objects)/len(retry_ids)*100 if retry_ids else 0,\n",
    "            \"retry_time_minutes\": (time.time() - start_time)/60\n",
    "        },\n",
    "        \"files_created\": {\n",
    "            \"recovered_objects\": final_filename if downloaded_objects else None,\n",
    "            \"log\": log_file,\n",
    "            \"still_failed_ids\": f\"still_failed_ids_{timestamp}.json\" if still_failed_ids else None,\n",
    "            \"still_not_found_ids\": f\"still_not_found_ids_{timestamp}.json\" if still_not_found_ids else None\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_file = f\"retry_summary_{timestamp}.json\"\n",
    "    with open(summary_file, 'w') as f:\n",
    "        json.dump(retry_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"📋 Retry summary saved: {summary_file}\")\n",
    "\n",
    "print(f\"\\n✅ Retry process complete at: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "56d95982",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE DOWNLOAD ANALYSIS ===\n",
      "Analysis started at: 2025-07-05 22:21:43.642699\n",
      "\n",
      "🧵 === TEXTILES ANALYSIS ===\n",
      "📊 Total Textiles Expected: 33,437\n",
      "  📁 met_textiles_batch_22800_20250705_134702.json: 22104 objects, 17749 with images\n",
      "  📁 idun/met_textiles_batch_11988_20250705_134921.json: 10354 objects, 8875 with images\n",
      "  📁 remaining_textiles_complete_20250705_135732.json: 736 objects, 647 with images\n",
      "\n",
      "📊 TEXTILES SUMMARY:\n",
      "  Total Expected: 33,437\n",
      "  Successfully Downloaded: 33,194\n",
      "  With Images: 27,271\n",
      "  Errors/Missing: 243\n",
      "  Success Rate: 99.3%\n",
      "\n",
      "🎨 === TAPESTRIES ANALYSIS ===\n",
      "📊 Total Tapestries Expected (missing from textiles): 151\n",
      "  📁 missing_tapestries_complete_20250705_191910.json: 150 objects, 102 with images\n",
      "\n",
      "📊 TAPESTRIES SUMMARY:\n",
      "  Total Expected: 151\n",
      "  Successfully Downloaded: 150\n",
      "  With Images: 102\n",
      "  Errors/Missing: 1\n",
      "  Success Rate: 99.3%\n",
      "\n",
      "🔗 === TEXTILES + TAPESTRIES COMBINED ===\n",
      "📊 COMBINED SUMMARY:\n",
      "  Total Expected: 33,588\n",
      "  Successfully Downloaded: 33,344\n",
      "  With Images: 27,373\n",
      "  Errors/Missing: 244\n",
      "  Overall Success Rate: 99.3%\n",
      "\n",
      "📁 Final summary saved to: final_download_summary_20250705_222144.json\n",
      "✅ Analysis complete at: 2025-07-05 22:21:44.396498\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_downloads():\n",
    "    print(\"=== COMPREHENSIVE DOWNLOAD ANALYSIS ===\")\n",
    "    print(f\"Analysis started at: {datetime.now()}\")\n",
    "    \n",
    "    # 1. TEXTILES ANALYSIS\n",
    "    print(\"\\n🧵 === TEXTILES ANALYSIS ===\")\n",
    "    \n",
    "    # Load original textile IDs\n",
    "    try:\n",
    "        with open(\"textile_object_ids.json\", \"r\") as f:\n",
    "            all_textile_ids = json.load(f)\n",
    "        total_textiles = len(all_textile_ids)\n",
    "        print(f\"📊 Total Textiles Expected: {total_textiles:,}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ textile_object_ids.json not found\")\n",
    "        return\n",
    "    \n",
    "    # Load all textile downloads\n",
    "    textile_files = [\n",
    "        'met_textiles_batch_22800_20250705_134702.json',\n",
    "        'idun/met_textiles_batch_11988_20250705_134921.json',\n",
    "        'remaining_textiles_complete_20250705_135732.json'\n",
    "    ]\n",
    "    \n",
    "    # Add any retry files if they exist\n",
    "    retry_files = [f for f in os.listdir('.') if f.startswith('retry_recovered_objects_')]\n",
    "    textile_files.extend(retry_files)\n",
    "    \n",
    "    downloaded_textile_ids = set()\n",
    "    textiles_with_images = 0\n",
    "    \n",
    "    for file in textile_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            file_ids = 0\n",
    "            file_images = 0\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    if obj['objectID'] not in downloaded_textile_ids:\n",
    "                        downloaded_textile_ids.add(obj['objectID'])\n",
    "                        file_ids += 1\n",
    "                        if obj.get('primaryImage'):\n",
    "                            textiles_with_images += 1\n",
    "                            file_images += 1\n",
    "            \n",
    "            print(f\"  📁 {file}: {file_ids} objects, {file_images} with images\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"  ❌ File not found: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    # Calculate textile errors\n",
    "    textile_errors = total_textiles - len(downloaded_textile_ids)\n",
    "    \n",
    "    print(f\"\\n📊 TEXTILES SUMMARY:\")\n",
    "    print(f\"  Total Expected: {total_textiles:,}\")\n",
    "    print(f\"  Successfully Downloaded: {len(downloaded_textile_ids):,}\")\n",
    "    print(f\"  With Images: {textiles_with_images:,}\")\n",
    "    print(f\"  Errors/Missing: {textile_errors:,}\")\n",
    "    print(f\"  Success Rate: {len(downloaded_textile_ids)/total_textiles*100:.1f}%\")\n",
    "    \n",
    "    # 2. TAPESTRIES ANALYSIS\n",
    "    print(\"\\n🎨 === TAPESTRIES ANALYSIS ===\")\n",
    "    \n",
    "    # Look for tapestry files\n",
    "    tapestry_files = [f for f in os.listdir('.') if f.startswith('missing_tapestries_complete_')]\n",
    "    \n",
    "    downloaded_tapestry_ids = set()\n",
    "    tapestries_with_images = 0\n",
    "    total_tapestries_expected = 0\n",
    "    \n",
    "    # Load tapestry summary to get expected count\n",
    "    summary_files = [f for f in os.listdir('.') if f.startswith('missing_tapestries_summary_')]\n",
    "    if summary_files:\n",
    "        try:\n",
    "            with open(summary_files[0], 'r') as f:\n",
    "                summary = json.load(f)\n",
    "            total_tapestries_expected = summary.get('analysis', {}).get('tapestries_missing_from_textiles', 0)\n",
    "            print(f\"📊 Total Tapestries Expected (missing from textiles): {total_tapestries_expected:,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading tapestry summary: {e}\")\n",
    "    \n",
    "    for file in tapestry_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            file_ids = 0\n",
    "            file_images = 0\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    if obj['objectID'] not in downloaded_tapestry_ids:\n",
    "                        downloaded_tapestry_ids.add(obj['objectID'])\n",
    "                        file_ids += 1\n",
    "                        if obj.get('primaryImage'):\n",
    "                            tapestries_with_images += 1\n",
    "                            file_images += 1\n",
    "            \n",
    "            print(f\"  📁 {file}: {file_ids} objects, {file_images} with images\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    # Calculate tapestry errors\n",
    "    tapestry_errors = total_tapestries_expected - len(downloaded_tapestry_ids) if total_tapestries_expected > 0 else 0\n",
    "    \n",
    "    print(f\"\\n📊 TAPESTRIES SUMMARY:\")\n",
    "    print(f\"  Total Expected: {total_tapestries_expected:,}\")\n",
    "    print(f\"  Successfully Downloaded: {len(downloaded_tapestry_ids):,}\")\n",
    "    print(f\"  With Images: {tapestries_with_images:,}\")\n",
    "    print(f\"  Errors/Missing: {tapestry_errors:,}\")\n",
    "    if total_tapestries_expected > 0:\n",
    "        print(f\"  Success Rate: {len(downloaded_tapestry_ids)/total_tapestries_expected*100:.1f}%\")\n",
    "    \n",
    "    # 3. COMBINED ANALYSIS\n",
    "    print(\"\\n🔗 === TEXTILES + TAPESTRIES COMBINED ===\")\n",
    "    \n",
    "    total_combined = len(downloaded_textile_ids) + len(downloaded_tapestry_ids)\n",
    "    total_images_combined = textiles_with_images + tapestries_with_images\n",
    "    total_expected_combined = total_textiles + total_tapestries_expected\n",
    "    total_errors_combined = textile_errors + tapestry_errors\n",
    "    \n",
    "    print(f\"📊 COMBINED SUMMARY:\")\n",
    "    print(f\"  Total Expected: {total_expected_combined:,}\")\n",
    "    print(f\"  Successfully Downloaded: {total_combined:,}\")\n",
    "    print(f\"  With Images: {total_images_combined:,}\")\n",
    "    print(f\"  Errors/Missing: {total_errors_combined:,}\")\n",
    "    if total_expected_combined > 0:\n",
    "        print(f\"  Overall Success Rate: {total_combined/total_expected_combined*100:.1f}%\")\n",
    "    \n",
    "    # 4. CREATE FINAL SUMMARY JSON\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_summary = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"textiles\": {\n",
    "            \"total_expected\": total_textiles,\n",
    "            \"successfully_downloaded\": len(downloaded_textile_ids),\n",
    "            \"with_images\": textiles_with_images,\n",
    "            \"errors_missing\": textile_errors,\n",
    "            \"success_rate_percent\": len(downloaded_textile_ids)/total_textiles*100 if total_textiles > 0 else 0\n",
    "        },\n",
    "        \"tapestries\": {\n",
    "            \"total_expected\": total_tapestries_expected,\n",
    "            \"successfully_downloaded\": len(downloaded_tapestry_ids),\n",
    "            \"with_images\": tapestries_with_images,\n",
    "            \"errors_missing\": tapestry_errors,\n",
    "            \"success_rate_percent\": len(downloaded_tapestry_ids)/total_tapestries_expected*100 if total_tapestries_expected > 0 else 0\n",
    "        },\n",
    "        \"combined\": {\n",
    "            \"total_expected\": total_expected_combined,\n",
    "            \"successfully_downloaded\": total_combined,\n",
    "            \"with_images\": total_images_combined,\n",
    "            \"errors_missing\": total_errors_combined,\n",
    "            \"success_rate_percent\": total_combined/total_expected_combined*100 if total_expected_combined > 0 else 0\n",
    "        },\n",
    "        \"files_analyzed\": {\n",
    "            \"textile_files\": textile_files,\n",
    "            \"tapestry_files\": tapestry_files\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_filename = f\"final_download_summary_{timestamp}.json\"\n",
    "    with open(summary_filename, 'w') as f:\n",
    "        json.dump(final_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n📁 Final summary saved to: {summary_filename}\")\n",
    "    print(f\"✅ Analysis complete at: {datetime.now()}\")\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "# Run the analysis\n",
    "summary = analyze_downloads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "02d68ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== COMPREHENSIVE DOWNLOAD ANALYSIS (INCLUDING REVERSE COMPLETE) ===\n",
      "Analysis started at: 2025-07-05 22:30:47.642878\n",
      "\n",
      "🧵 === TEXTILES ANALYSIS ===\n",
      "📊 Total Textiles Expected: 33,437\n",
      "  📁 met_textiles_batch_22800_20250705_134702.json: 22104 unique objects, 17749 with images\n",
      "  📁 idun/met_textiles_batch_11988_20250705_134921.json: 10354 unique objects, 8875 with images\n",
      "  📁 remaining_textiles_complete_20250705_135732.json: 736 unique objects, 647 with images\n",
      "  📁 idun/met_textiles_complete_reverse_20250705_222820.json: 0 unique objects, 0 with images\n",
      "\n",
      "📊 TEXTILES SUMMARY:\n",
      "  Total Expected: 33,437\n",
      "  Successfully Downloaded: 33,194\n",
      "  With Images: 27,271\n",
      "  Errors/Missing: 243\n",
      "  Success Rate: 99.3%\n",
      "\n",
      "🎨 === TAPESTRIES ANALYSIS ===\n",
      "📊 Total Tapestries Expected (missing from textiles): 151\n",
      "  📁 missing_tapestries_complete_20250705_191910.json: 150 objects, 102 with images\n",
      "\n",
      "📊 TAPESTRIES SUMMARY:\n",
      "  Total Expected: 151\n",
      "  Successfully Downloaded: 150\n",
      "  With Images: 102\n",
      "  Errors/Missing: 1\n",
      "  Success Rate: 99.3%\n",
      "\n",
      "🔗 === TEXTILES + TAPESTRIES COMBINED ===\n",
      "📊 COMBINED SUMMARY:\n",
      "  Total Expected: 33,588\n",
      "  Successfully Downloaded: 33,344\n",
      "  With Images: 27,373\n",
      "  Errors/Missing: 244\n",
      "  Overall Success Rate: 99.3%\n",
      "\n",
      "🔄 === REVERSE COMPLETE FILE ANALYSIS ===\n",
      "  📁 Reverse Complete File:\n",
      "    Total objects: 32,147\n",
      "    Objects with images: 26,406\n",
      "    Image percentage: 82.1%\n",
      "    New objects not in other files: 32,147\n",
      "    🎉 This file fills 32147 gaps!\n",
      "\n",
      "📁 Updated final summary saved to: final_download_summary_with_reverse_20250705_223049.json\n",
      "✅ Analysis complete at: 2025-07-05 22:30:49.348622\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_all_downloads_including_reverse():\n",
    "    print(\"=== COMPREHENSIVE DOWNLOAD ANALYSIS (INCLUDING REVERSE COMPLETE) ===\")\n",
    "    print(f\"Analysis started at: {datetime.now()}\")\n",
    "    \n",
    "    # 1. TEXTILES ANALYSIS\n",
    "    print(\"\\n🧵 === TEXTILES ANALYSIS ===\")\n",
    "    \n",
    "    # Load original textile IDs\n",
    "    try:\n",
    "        with open(\"textile_object_ids.json\", \"r\") as f:\n",
    "            all_textile_ids = json.load(f)\n",
    "        total_textiles = len(all_textile_ids)\n",
    "        print(f\"📊 Total Textiles Expected: {total_textiles:,}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ textile_object_ids.json not found\")\n",
    "        return\n",
    "    \n",
    "    # Load all textile downloads INCLUDING the new reverse complete file\n",
    "    textile_files = [\n",
    "        'met_textiles_batch_22800_20250705_134702.json',\n",
    "        'idun/met_textiles_batch_11988_20250705_134921.json',\n",
    "        'remaining_textiles_complete_20250705_135732.json',\n",
    "        'idun/met_textiles_complete_reverse_20250705_222820.json'  # NEW FILE!\n",
    "    ]\n",
    "    \n",
    "    # Add any retry files if they exist\n",
    "    retry_files = [f for f in os.listdir('.') if f.startswith('retry_recovered_objects_')]\n",
    "    textile_files.extend(retry_files)\n",
    "    \n",
    "    downloaded_textile_ids = set()\n",
    "    textiles_with_images = 0\n",
    "    \n",
    "    for file in textile_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            file_ids = 0\n",
    "            file_images = 0\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    if obj['objectID'] not in downloaded_textile_ids:\n",
    "                        downloaded_textile_ids.add(obj['objectID'])\n",
    "                        file_ids += 1\n",
    "                        if obj.get('primaryImage'):\n",
    "                            textiles_with_images += 1\n",
    "                            file_images += 1\n",
    "            \n",
    "            print(f\"  📁 {file}: {file_ids} unique objects, {file_images} with images\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"  ❌ File not found: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    # Calculate textile errors\n",
    "    textile_errors = total_textiles - len(downloaded_textile_ids)\n",
    "    \n",
    "    print(f\"\\n📊 TEXTILES SUMMARY:\")\n",
    "    print(f\"  Total Expected: {total_textiles:,}\")\n",
    "    print(f\"  Successfully Downloaded: {len(downloaded_textile_ids):,}\")\n",
    "    print(f\"  With Images: {textiles_with_images:,}\")\n",
    "    print(f\"  Errors/Missing: {textile_errors:,}\")\n",
    "    print(f\"  Success Rate: {len(downloaded_textile_ids)/total_textiles*100:.1f}%\")\n",
    "    \n",
    "    # 2. TAPESTRIES ANALYSIS\n",
    "    print(\"\\n🎨 === TAPESTRIES ANALYSIS ===\")\n",
    "    \n",
    "    # Look for tapestry files\n",
    "    tapestry_files = [f for f in os.listdir('.') if f.startswith('missing_tapestries_complete_')]\n",
    "    \n",
    "    downloaded_tapestry_ids = set()\n",
    "    tapestries_with_images = 0\n",
    "    total_tapestries_expected = 0\n",
    "    \n",
    "    # Load tapestry summary to get expected count\n",
    "    summary_files = [f for f in os.listdir('.') if f.startswith('missing_tapestries_summary_')]\n",
    "    if summary_files:\n",
    "        try:\n",
    "            with open(summary_files[0], 'r') as f:\n",
    "                summary = json.load(f)\n",
    "            total_tapestries_expected = summary.get('analysis', {}).get('tapestries_missing_from_textiles', 0)\n",
    "            print(f\"📊 Total Tapestries Expected (missing from textiles): {total_tapestries_expected:,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading tapestry summary: {e}\")\n",
    "    \n",
    "    for file in tapestry_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            file_ids = 0\n",
    "            file_images = 0\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    if obj['objectID'] not in downloaded_tapestry_ids:\n",
    "                        downloaded_tapestry_ids.add(obj['objectID'])\n",
    "                        file_ids += 1\n",
    "                        if obj.get('primaryImage'):\n",
    "                            tapestries_with_images += 1\n",
    "                            file_images += 1\n",
    "            \n",
    "            print(f\"  📁 {file}: {file_ids} objects, {file_images} with images\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    # Calculate tapestry errors\n",
    "    tapestry_errors = total_tapestries_expected - len(downloaded_tapestry_ids) if total_tapestries_expected > 0 else 0\n",
    "    \n",
    "    print(f\"\\n📊 TAPESTRIES SUMMARY:\")\n",
    "    print(f\"  Total Expected: {total_tapestries_expected:,}\")\n",
    "    print(f\"  Successfully Downloaded: {len(downloaded_tapestry_ids):,}\")\n",
    "    print(f\"  With Images: {tapestries_with_images:,}\")\n",
    "    print(f\"  Errors/Missing: {tapestry_errors:,}\")\n",
    "    if total_tapestries_expected > 0:\n",
    "        print(f\"  Success Rate: {len(downloaded_tapestry_ids)/total_tapestries_expected*100:.1f}%\")\n",
    "    \n",
    "    # 3. COMBINED ANALYSIS\n",
    "    print(\"\\n🔗 === TEXTILES + TAPESTRIES COMBINED ===\")\n",
    "    \n",
    "    total_combined = len(downloaded_textile_ids) + len(downloaded_tapestry_ids)\n",
    "    total_images_combined = textiles_with_images + tapestries_with_images\n",
    "    total_expected_combined = total_textiles + total_tapestries_expected\n",
    "    total_errors_combined = textile_errors + tapestry_errors\n",
    "    \n",
    "    print(f\"📊 COMBINED SUMMARY:\")\n",
    "    print(f\"  Total Expected: {total_expected_combined:,}\")\n",
    "    print(f\"  Successfully Downloaded: {total_combined:,}\")\n",
    "    print(f\"  With Images: {total_images_combined:,}\")\n",
    "    print(f\"  Errors/Missing: {total_errors_combined:,}\")\n",
    "    if total_expected_combined > 0:\n",
    "        print(f\"  Overall Success Rate: {total_combined/total_expected_combined*100:.1f}%\")\n",
    "    \n",
    "    # 4. DETAILED ANALYSIS OF THE NEW REVERSE FILE\n",
    "    print(\"\\n🔄 === REVERSE COMPLETE FILE ANALYSIS ===\")\n",
    "    try:\n",
    "        with open('idun/met_textiles_complete_reverse_20250705_222820.json', 'r', encoding='utf-8') as f:\n",
    "            reverse_complete_data = json.load(f)\n",
    "        \n",
    "        reverse_complete_ids = set()\n",
    "        reverse_complete_images = 0\n",
    "        \n",
    "        for obj in reverse_complete_data:\n",
    "            if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                reverse_complete_ids.add(obj['objectID'])\n",
    "                if obj.get('primaryImage'):\n",
    "                    reverse_complete_images += 1\n",
    "        \n",
    "        print(f\"  📁 Reverse Complete File:\")\n",
    "        print(f\"    Total objects: {len(reverse_complete_ids):,}\")\n",
    "        print(f\"    Objects with images: {reverse_complete_images:,}\")\n",
    "        print(f\"    Image percentage: {reverse_complete_images/len(reverse_complete_ids)*100:.1f}%\")\n",
    "        \n",
    "        # Check if this file fills any gaps\n",
    "        all_other_textile_ids = downloaded_textile_ids - reverse_complete_ids\n",
    "        missing_filled = len(reverse_complete_ids - all_other_textile_ids)\n",
    "        \n",
    "        print(f\"    New objects not in other files: {missing_filled:,}\")\n",
    "        \n",
    "        if missing_filled > 0:\n",
    "            print(f\"    🎉 This file fills {missing_filled} gaps!\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"  ❌ Error analyzing reverse complete file: {e}\")\n",
    "    \n",
    "    # 5. CHECK FOR PERFECT COMPLETION\n",
    "    if textile_errors == 0:\n",
    "        print(f\"\\n🎉 PERFECT! ALL TEXTILE OBJECTS DOWNLOADED! 🎉\")\n",
    "    elif textile_errors < 10:\n",
    "        print(f\"\\n✅ NEARLY PERFECT! Only {textile_errors} objects missing!\")\n",
    "    \n",
    "    # 6. CREATE UPDATED FINAL SUMMARY JSON\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    final_summary = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"textiles\": {\n",
    "            \"total_expected\": total_textiles,\n",
    "            \"successfully_downloaded\": len(downloaded_textile_ids),\n",
    "            \"with_images\": textiles_with_images,\n",
    "            \"errors_missing\": textile_errors,\n",
    "            \"success_rate_percent\": len(downloaded_textile_ids)/total_textiles*100 if total_textiles > 0 else 0\n",
    "        },\n",
    "        \"tapestries\": {\n",
    "            \"total_expected\": total_tapestries_expected,\n",
    "            \"successfully_downloaded\": len(downloaded_tapestry_ids),\n",
    "            \"with_images\": tapestries_with_images,\n",
    "            \"errors_missing\": tapestry_errors,\n",
    "            \"success_rate_percent\": len(downloaded_tapestry_ids)/total_tapestries_expected*100 if total_tapestries_expected > 0 else 0\n",
    "        },\n",
    "        \"combined\": {\n",
    "            \"total_expected\": total_expected_combined,\n",
    "            \"successfully_downloaded\": total_combined,\n",
    "            \"with_images\": total_images_combined,\n",
    "            \"errors_missing\": total_errors_combined,\n",
    "            \"success_rate_percent\": total_combined/total_expected_combined*100 if total_expected_combined > 0 else 0\n",
    "        },\n",
    "        \"files_analyzed\": {\n",
    "            \"textile_files\": textile_files,\n",
    "            \"tapestry_files\": tapestry_files\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    summary_filename = f\"final_download_summary_with_reverse_{timestamp}.json\"\n",
    "    with open(summary_filename, 'w') as f:\n",
    "        json.dump(final_summary, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n📁 Updated final summary saved to: {summary_filename}\")\n",
    "    print(f\"✅ Analysis complete at: {datetime.now()}\")\n",
    "    \n",
    "    return final_summary\n",
    "\n",
    "# Run the updated analysis\n",
    "summary = analyze_all_downloads_including_reverse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "eddc0414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CORRECTED COMPREHENSIVE DOWNLOAD ANALYSIS ===\n",
      "Analysis started at: 2025-07-05 22:33:27.586608\n",
      "\n",
      "🧵 === TEXTILES ANALYSIS ===\n",
      "📊 Total Textiles Expected: 33,437\n",
      "  📁 met_textiles_batch_22800_20250705_134702.json:\n",
      "      Total in file: 22,104\n",
      "      New objects: 22,104\n",
      "      New with images: 17,749\n",
      "      Duplicates: 0\n",
      "  📁 idun/met_textiles_batch_11988_20250705_134921.json:\n",
      "      Total in file: 20,497\n",
      "      New objects: 10,354\n",
      "      New with images: 8,875\n",
      "      Duplicates: 10,143\n",
      "  📁 remaining_textiles_complete_20250705_135732.json:\n",
      "      Total in file: 736\n",
      "      New objects: 736\n",
      "      New with images: 647\n",
      "      Duplicates: 0\n",
      "  📁 idun/met_textiles_complete_reverse_20250705_222820.json:\n",
      "      Total in file: 32,147\n",
      "      New objects: 0\n",
      "      New with images: 0\n",
      "      Duplicates: 32,147\n",
      "\n",
      "📊 CORRECTED TEXTILES SUMMARY:\n",
      "  Total Expected: 33,437\n",
      "  Successfully Downloaded: 33,194\n",
      "  With Images: 27,271\n",
      "  Errors/Missing: 243\n",
      "  Success Rate: 99.3%\n",
      "\n",
      "🎨 === TAPESTRIES ANALYSIS ===\n",
      "📊 Total Tapestries Expected (missing from textiles): 151\n",
      "  📁 missing_tapestries_complete_20250705_191910.json: 150 objects, 102 with images\n",
      "\n",
      "📊 TAPESTRIES SUMMARY:\n",
      "  Total Expected: 151\n",
      "  Successfully Downloaded: 150\n",
      "  With Images: 102\n",
      "  Errors/Missing: 1\n",
      "  Success Rate: 99.3%\n",
      "\n",
      "🔗 === CORRECTED TEXTILES + TAPESTRIES COMBINED ===\n",
      "📊 CORRECTED COMBINED SUMMARY:\n",
      "  Total Expected: 33,588\n",
      "  Successfully Downloaded: 33,344\n",
      "  With Images: 27,373\n",
      "  Errors/Missing: 244\n",
      "  Overall Success Rate: 99.3%\n",
      "\n",
      "📋 === DETAILED FILE BREAKDOWN ===\n",
      "  📁 met_textiles_batch_22800_20250705_134702.json:\n",
      "      Contributed 22,104 unique objects\n",
      "      Had 0 duplicates\n",
      "      Added 17,749 new images\n",
      "  📁 idun/met_textiles_batch_11988_20250705_134921.json:\n",
      "      Contributed 10,354 unique objects\n",
      "      Had 10,143 duplicates\n",
      "      Added 8,875 new images\n",
      "  📁 remaining_textiles_complete_20250705_135732.json:\n",
      "      Contributed 736 unique objects\n",
      "      Had 0 duplicates\n",
      "      Added 647 new images\n",
      "  📁 idun/met_textiles_complete_reverse_20250705_222820.json:\n",
      "      Contributed 0 unique objects\n",
      "      Had 32,147 duplicates\n",
      "      Added 0 new images\n",
      "\n",
      "🔄 === REVERSE FILE SPECIFIC IMPACT ===\n",
      "  The reverse file contributed:\n",
      "  📈 0 NEW unique objects\n",
      "  🖼️  0 NEW objects with images\n",
      "  🔄 32,147 objects already downloaded\n",
      "  ℹ️  The reverse file contained no new objects\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def analyze_all_downloads_with_correct_logic():\n",
    "    print(\"=== CORRECTED COMPREHENSIVE DOWNLOAD ANALYSIS ===\")\n",
    "    print(f\"Analysis started at: {datetime.now()}\")\n",
    "    \n",
    "    # 1. TEXTILES ANALYSIS\n",
    "    print(\"\\n🧵 === TEXTILES ANALYSIS ===\")\n",
    "    \n",
    "    # Load original textile IDs\n",
    "    try:\n",
    "        with open(\"textile_object_ids.json\", \"r\") as f:\n",
    "            all_textile_ids = json.load(f)\n",
    "        total_textiles = len(all_textile_ids)\n",
    "        print(f\"📊 Total Textiles Expected: {total_textiles:,}\")\n",
    "    except FileNotFoundError:\n",
    "        print(\"❌ textile_object_ids.json not found\")\n",
    "        return\n",
    "    \n",
    "    # Load all textile downloads INCLUDING the reverse complete file\n",
    "    textile_files = [\n",
    "        'met_textiles_batch_22800_20250705_134702.json',\n",
    "        'idun/met_textiles_batch_11988_20250705_134921.json', \n",
    "        'remaining_textiles_complete_20250705_135732.json',\n",
    "        'idun/met_textiles_complete_reverse_20250705_222820.json'\n",
    "    ]\n",
    "    \n",
    "    # Add any retry files if they exist\n",
    "    retry_files = [f for f in os.listdir('.') if f.startswith('retry_recovered_objects_')]\n",
    "    textile_files.extend(retry_files)\n",
    "    \n",
    "    downloaded_textile_ids = set()\n",
    "    textiles_with_images = 0\n",
    "    \n",
    "    # Track each file's contribution\n",
    "    file_contributions = {}\n",
    "    \n",
    "    for file in textile_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            file_ids = set()\n",
    "            file_images = 0\n",
    "            \n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    file_ids.add(obj['objectID'])\n",
    "                    if obj.get('primaryImage'):\n",
    "                        file_images += 1\n",
    "            \n",
    "            # Count NEW objects from this file\n",
    "            new_objects = file_ids - downloaded_textile_ids\n",
    "            new_images = 0\n",
    "            \n",
    "            # Count images in new objects only\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    if obj['objectID'] in new_objects and obj.get('primaryImage'):\n",
    "                        new_images += 1\n",
    "            \n",
    "            # Add new objects to total\n",
    "            downloaded_textile_ids.update(new_objects)\n",
    "            textiles_with_images += new_images\n",
    "            \n",
    "            file_contributions[file] = {\n",
    "                'total_in_file': len(file_ids),\n",
    "                'new_objects': len(new_objects),\n",
    "                'new_images': new_images,\n",
    "                'duplicates': len(file_ids) - len(new_objects)\n",
    "            }\n",
    "            \n",
    "            print(f\"  📁 {file}:\")\n",
    "            print(f\"      Total in file: {len(file_ids):,}\")\n",
    "            print(f\"      New objects: {len(new_objects):,}\")\n",
    "            print(f\"      New with images: {new_images:,}\")\n",
    "            print(f\"      Duplicates: {len(file_ids) - len(new_objects):,}\")\n",
    "            \n",
    "        except FileNotFoundError:\n",
    "            print(f\"  ❌ File not found: {file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    # Calculate textile errors\n",
    "    textile_errors = total_textiles - len(downloaded_textile_ids)\n",
    "    \n",
    "    print(f\"\\n📊 CORRECTED TEXTILES SUMMARY:\")\n",
    "    print(f\"  Total Expected: {total_textiles:,}\")\n",
    "    print(f\"  Successfully Downloaded: {len(downloaded_textile_ids):,}\")\n",
    "    print(f\"  With Images: {textiles_with_images:,}\")\n",
    "    print(f\"  Errors/Missing: {textile_errors:,}\")\n",
    "    print(f\"  Success Rate: {len(downloaded_textile_ids)/total_textiles*100:.1f}%\")\n",
    "    \n",
    "    # 2. TAPESTRIES ANALYSIS (unchanged)\n",
    "    print(\"\\n🎨 === TAPESTRIES ANALYSIS ===\")\n",
    "    \n",
    "    tapestry_files = [f for f in os.listdir('.') if f.startswith('missing_tapestries_complete_')]\n",
    "    \n",
    "    downloaded_tapestry_ids = set()\n",
    "    tapestries_with_images = 0\n",
    "    total_tapestries_expected = 0\n",
    "    \n",
    "    summary_files = [f for f in os.listdir('.') if f.startswith('missing_tapestries_summary_')]\n",
    "    if summary_files:\n",
    "        try:\n",
    "            with open(summary_files[0], 'r') as f:\n",
    "                summary = json.load(f)\n",
    "            total_tapestries_expected = summary.get('analysis', {}).get('tapestries_missing_from_textiles', 0)\n",
    "            print(f\"📊 Total Tapestries Expected (missing from textiles): {total_tapestries_expected:,}\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading tapestry summary: {e}\")\n",
    "    \n",
    "    for file in tapestry_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            file_ids = 0\n",
    "            file_images = 0\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    if obj['objectID'] not in downloaded_tapestry_ids:\n",
    "                        downloaded_tapestry_ids.add(obj['objectID'])\n",
    "                        file_ids += 1\n",
    "                        if obj.get('primaryImage'):\n",
    "                            tapestries_with_images += 1\n",
    "                            file_images += 1\n",
    "            \n",
    "            print(f\"  📁 {file}: {file_ids} objects, {file_images} with images\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    tapestry_errors = total_tapestries_expected - len(downloaded_tapestry_ids) if total_tapestries_expected > 0 else 0\n",
    "    \n",
    "    print(f\"\\n📊 TAPESTRIES SUMMARY:\")\n",
    "    print(f\"  Total Expected: {total_tapestries_expected:,}\")\n",
    "    print(f\"  Successfully Downloaded: {len(downloaded_tapestry_ids):,}\")\n",
    "    print(f\"  With Images: {tapestries_with_images:,}\")\n",
    "    print(f\"  Errors/Missing: {tapestry_errors:,}\")\n",
    "    if total_tapestries_expected > 0:\n",
    "        print(f\"  Success Rate: {len(downloaded_tapestry_ids)/total_tapestries_expected*100:.1f}%\")\n",
    "    \n",
    "    # 3. COMBINED ANALYSIS\n",
    "    print(\"\\n🔗 === CORRECTED TEXTILES + TAPESTRIES COMBINED ===\")\n",
    "    \n",
    "    total_combined = len(downloaded_textile_ids) + len(downloaded_tapestry_ids)\n",
    "    total_images_combined = textiles_with_images + tapestries_with_images\n",
    "    total_expected_combined = total_textiles + total_tapestries_expected\n",
    "    total_errors_combined = textile_errors + tapestry_errors\n",
    "    \n",
    "    print(f\"📊 CORRECTED COMBINED SUMMARY:\")\n",
    "    print(f\"  Total Expected: {total_expected_combined:,}\")\n",
    "    print(f\"  Successfully Downloaded: {total_combined:,}\")\n",
    "    print(f\"  With Images: {total_images_combined:,}\")\n",
    "    print(f\"  Errors/Missing: {total_errors_combined:,}\")\n",
    "    if total_expected_combined > 0:\n",
    "        print(f\"  Overall Success Rate: {total_combined/total_expected_combined*100:.1f}%\")\n",
    "    \n",
    "    # 4. DETAILED FILE BREAKDOWN\n",
    "    print(f\"\\n📋 === DETAILED FILE BREAKDOWN ===\")\n",
    "    for file, stats in file_contributions.items():\n",
    "        print(f\"  📁 {file}:\")\n",
    "        print(f\"      Contributed {stats['new_objects']:,} unique objects\")\n",
    "        print(f\"      Had {stats['duplicates']:,} duplicates\")\n",
    "        print(f\"      Added {stats['new_images']:,} new images\")\n",
    "    \n",
    "    # 5. CHECK FOR PERFECT COMPLETION\n",
    "    if textile_errors == 0:\n",
    "        print(f\"\\n🎉 PERFECT! ALL TEXTILE OBJECTS DOWNLOADED! 🎉\")\n",
    "    elif textile_errors < 10:\n",
    "        print(f\"\\n✅ NEARLY PERFECT! Only {textile_errors} objects missing!\")\n",
    "    \n",
    "    # 6. REVERSE FILE SPECIFIC ANALYSIS\n",
    "    print(f\"\\n🔄 === REVERSE FILE SPECIFIC IMPACT ===\")\n",
    "    reverse_file = 'idun/met_textiles_complete_reverse_20250705_222820.json'\n",
    "    if reverse_file in file_contributions:\n",
    "        reverse_stats = file_contributions[reverse_file]\n",
    "        print(f\"  The reverse file contributed:\")\n",
    "        print(f\"  📈 {reverse_stats['new_objects']:,} NEW unique objects\")\n",
    "        print(f\"  🖼️  {reverse_stats['new_images']:,} NEW objects with images\")\n",
    "        print(f\"  🔄 {reverse_stats['duplicates']:,} objects already downloaded\")\n",
    "        \n",
    "        if reverse_stats['new_objects'] > 0:\n",
    "            print(f\"  🎉 The reverse file significantly improved coverage!\")\n",
    "        else:\n",
    "            print(f\"  ℹ️  The reverse file contained no new objects\")\n",
    "    \n",
    "    return {\n",
    "        \"total_textiles\": len(downloaded_textile_ids),\n",
    "        \"total_tapestries\": len(downloaded_tapestry_ids),\n",
    "        \"total_combined\": total_combined,\n",
    "        \"total_images\": total_images_combined,\n",
    "        \"success_rate\": total_combined/total_expected_combined*100 if total_expected_combined > 0 else 0,\n",
    "        \"file_contributions\": file_contributions\n",
    "    }\n",
    "\n",
    "# Run the corrected analysis\n",
    "corrected_summary = analyze_all_downloads_with_correct_logic()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "021176ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINDING THE 244 MISSING OBJECTS ===\n",
      "Analysis started at: 2025-07-05 22:35:32.441249\n",
      "\n",
      "🔍 === FINDING MISSING TEXTILES ===\n",
      "📊 Missing textiles: 243\n",
      "📋 Missing textile IDs: [21107, 72339, 77239, 77240, 77241, 77242, 212220, 212221, 212223, 212224, 212225, 212226, 212227, 212228, 212229, 212230, 212231, 212232, 212233, 212234]\n",
      "\n",
      "🔍 === FINDING MISSING TAPESTRIES ===\n",
      "📊 Expected tapestries: 151\n",
      "📊 Downloaded tapestries: 150\n",
      "📊 Missing tapestries: 1\n",
      "\n",
      "🔍 === CHECKING FAILED/NOT FOUND FILES ===\n",
      "📁 not_found_ids_20250705_135732.json: 243 failed IDs\n",
      "📁 missing_tapestries_failed_20250705_191910.json: Not found\n",
      "📁 missing_tapestries_not_found_20250705_191910.json: 1 failed IDs\n",
      "📊 Total unique failed IDs across all files: 244\n",
      "\n",
      "🔗 === CROSS-REFERENCING MISSING WITH FAILED ===\n",
      "📊 Missing textiles that are in failed files: 243\n",
      "📊 Missing textiles NOT in failed files: 0\n",
      "\n",
      "📁 Final missing objects analysis saved to: final_missing_objects_20250705_223533.json\n",
      "✅ Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def find_missing_objects():\n",
    "    print(\"=== FINDING THE 244 MISSING OBJECTS ===\")\n",
    "    print(f\"Analysis started at: {datetime.now()}\")\n",
    "    \n",
    "    # 1. FIND MISSING TEXTILES (243 objects)\n",
    "    print(\"\\n🔍 === FINDING MISSING TEXTILES ===\")\n",
    "    \n",
    "    # Load expected textile IDs\n",
    "    with open(\"textile_object_ids.json\", \"r\") as f:\n",
    "        all_textile_ids = set(json.load(f))\n",
    "    \n",
    "    # Load all downloaded textile IDs\n",
    "    textile_files = [\n",
    "        'met_textiles_batch_22800_20250705_134702.json',\n",
    "        'idun/met_textiles_batch_11988_20250705_134921.json', \n",
    "        'remaining_textiles_complete_20250705_135732.json',\n",
    "        'idun/met_textiles_complete_reverse_20250705_222820.json'\n",
    "    ]\n",
    "    \n",
    "    downloaded_textile_ids = set()\n",
    "    \n",
    "    for file in textile_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    downloaded_textile_ids.add(obj['objectID'])\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    missing_textiles = sorted(list(all_textile_ids - downloaded_textile_ids))\n",
    "    print(f\"📊 Missing textiles: {len(missing_textiles)}\")\n",
    "    print(f\"📋 Missing textile IDs: {missing_textiles[:20]}\")  # Show first 20\n",
    "    \n",
    "    # 2. FIND MISSING TAPESTRIES (1 object)\n",
    "    print(\"\\n🔍 === FINDING MISSING TAPESTRIES ===\")\n",
    "    \n",
    "    # Load expected tapestry IDs from summary\n",
    "    try:\n",
    "        with open('missing_tapestries_summary_20250705_191910.json', 'r') as f:\n",
    "            tapestry_summary = json.load(f)\n",
    "        expected_tapestries = tapestry_summary['analysis']['tapestries_missing_from_textiles']\n",
    "        print(f\"📊 Expected tapestries: {expected_tapestries}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading tapestry summary: {e}\")\n",
    "        return\n",
    "    \n",
    "    # Load downloaded tapestries\n",
    "    try:\n",
    "        with open('missing_tapestries_complete_20250705_191910.json', 'r', encoding='utf-8') as f:\n",
    "            tapestry_data = json.load(f)\n",
    "        downloaded_tapestries = {obj['objectID'] for obj in tapestry_data if isinstance(obj, dict) and 'objectID' in obj}\n",
    "        print(f\"📊 Downloaded tapestries: {len(downloaded_tapestries)}\")\n",
    "        \n",
    "        # This should be 1 missing tapestry but we need the original tapestry IDs to find which one\n",
    "        print(f\"📊 Missing tapestries: {expected_tapestries - len(downloaded_tapestries)}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading tapestry data: {e}\")\n",
    "    \n",
    "    # 3. CHECK FAILED/NOT FOUND FILES\n",
    "    print(\"\\n🔍 === CHECKING FAILED/NOT FOUND FILES ===\")\n",
    "    \n",
    "    failed_files = [\n",
    "        'failed_ids_20250705_135732.json',\n",
    "        'not_found_ids_20250705_135732.json',\n",
    "        'missing_tapestries_failed_20250705_191910.json',\n",
    "        'missing_tapestries_not_found_20250705_191910.json'\n",
    "    ]\n",
    "    \n",
    "    all_failed_ids = set()\n",
    "    \n",
    "    for file in failed_files:\n",
    "        try:\n",
    "            with open(file, 'r') as f:\n",
    "                failed_ids = json.load(f)\n",
    "            if failed_ids:\n",
    "                all_failed_ids.update(failed_ids)\n",
    "                print(f\"📁 {file}: {len(failed_ids)} failed IDs\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"📁 {file}: Not found\")\n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    print(f\"📊 Total unique failed IDs across all files: {len(all_failed_ids)}\")\n",
    "    \n",
    "    # 4. CROSS-REFERENCE MISSING WITH FAILED\n",
    "    print(\"\\n🔗 === CROSS-REFERENCING MISSING WITH FAILED ===\")\n",
    "    \n",
    "    missing_in_failed = set(missing_textiles) & all_failed_ids\n",
    "    missing_not_in_failed = set(missing_textiles) - all_failed_ids\n",
    "    \n",
    "    print(f\"📊 Missing textiles that are in failed files: {len(missing_in_failed)}\")\n",
    "    print(f\"📊 Missing textiles NOT in failed files: {len(missing_not_in_failed)}\")\n",
    "    \n",
    "    if missing_not_in_failed:\n",
    "        print(f\"📋 Missing textiles NOT in failed files: {sorted(list(missing_not_in_failed))[:10]}\")\n",
    "    \n",
    "    # 5. SAVE FINAL MISSING LIST FOR RETRY\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    final_missing = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"summary\": {\n",
    "            \"total_missing\": len(missing_textiles),\n",
    "            \"missing_in_failed_files\": len(missing_in_failed),\n",
    "            \"missing_not_in_failed_files\": len(missing_not_in_failed)\n",
    "        },\n",
    "        \"missing_textile_ids\": missing_textiles,\n",
    "        \"missing_in_failed\": sorted(list(missing_in_failed)),\n",
    "        \"missing_not_in_failed\": sorted(list(missing_not_in_failed))\n",
    "    }\n",
    "    \n",
    "    filename = f\"final_missing_objects_{timestamp}.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(final_missing, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n📁 Final missing objects analysis saved to: {filename}\")\n",
    "    print(f\"✅ Analysis complete!\")\n",
    "    \n",
    "    return final_missing\n",
    "\n",
    "# Find the missing objects\n",
    "missing_analysis = find_missing_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72f3f6f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CHECKING IF MISSING OBJECTS ARE IN REVERSE FILE ===\n",
      "Analysis started at: 2025-07-05 22:37:36.752778\n",
      "📊 Missing textile IDs to check: 243\n",
      "📋 Sample missing IDs: [21107, 72339, 77239, 77240, 77241, 77242, 212220, 212221, 212223, 212224]\n",
      "\n",
      "🔍 === CHECKING REVERSE FILE ===\n",
      "📊 Total objects in reverse file: 32,147\n",
      "🎯 Missing objects found in reverse file: 0\n",
      "❌ No missing objects found in reverse file\n",
      "\n",
      "🔍 === DOUBLE-CHECK: RECOUNT WITH ALL FILES INCLUDING REVERSE ===\n",
      "📊 Total expected textile objects: 33,437\n",
      "📁 met_textiles_batch_22800_20250705_134702.json: 22,104 total, 22,104 new\n",
      "📁 idun/met_textiles_batch_11988_20250705_134921.json: 20,497 total, 10,354 new\n",
      "📁 remaining_textiles_complete_20250705_135732.json: 736 total, 736 new\n",
      "📁 idun/met_textiles_complete_reverse_20250705_222820.json: 32,147 total, 0 new\n",
      "\n",
      "📊 FINAL RECOUNT RESULTS:\n",
      "  Expected: 33,437\n",
      "  Downloaded: 33,194\n",
      "  Actually Missing: 243\n",
      "  Success Rate: 99.273%\n",
      "\n",
      "📋 Truly missing IDs (first 20): [21107, 72339, 77239, 77240, 77241, 77242, 212220, 212221, 212223, 212224, 212225, 212226, 212227, 212228, 212229, 212230, 212231, 212232, 212233, 212234]\n",
      "\n",
      "📁 Corrected analysis saved to: corrected_missing_analysis_20250705_223738.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def check_missing_in_reverse_file():\n",
    "    print(\"=== CHECKING IF MISSING OBJECTS ARE IN REVERSE FILE ===\")\n",
    "    print(f\"Analysis started at: {datetime.now()}\")\n",
    "    \n",
    "    # 1. Load the missing objects from the analysis\n",
    "    try:\n",
    "        with open('final_missing_objects_20250705_223533.json', 'r') as f:\n",
    "            missing_data = json.load(f)\n",
    "        missing_textile_ids = set(missing_data['missing_textile_ids'])\n",
    "        print(f\"📊 Missing textile IDs to check: {len(missing_textile_ids)}\")\n",
    "        print(f\"📋 Sample missing IDs: {sorted(list(missing_textile_ids))[:10]}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading missing objects file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 2. Load the reverse file and check for these IDs\n",
    "    print(f\"\\n🔍 === CHECKING REVERSE FILE ===\")\n",
    "    try:\n",
    "        with open('idun/met_textiles_complete_reverse_20250705_222820.json', 'r', encoding='utf-8') as f:\n",
    "            reverse_data = json.load(f)\n",
    "        \n",
    "        reverse_ids = set()\n",
    "        found_missing_in_reverse = set()\n",
    "        \n",
    "        for obj in reverse_data:\n",
    "            if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                obj_id = obj['objectID']\n",
    "                reverse_ids.add(obj_id)\n",
    "                \n",
    "                # Check if this object is in our missing list\n",
    "                if obj_id in missing_textile_ids:\n",
    "                    found_missing_in_reverse.add(obj_id)\n",
    "        \n",
    "        print(f\"📊 Total objects in reverse file: {len(reverse_ids):,}\")\n",
    "        print(f\"🎯 Missing objects found in reverse file: {len(found_missing_in_reverse)}\")\n",
    "        \n",
    "        if found_missing_in_reverse:\n",
    "            print(f\"✅ FOUND! These missing objects ARE in the reverse file:\")\n",
    "            print(f\"📋 Found IDs: {sorted(list(found_missing_in_reverse))[:20]}\")\n",
    "            \n",
    "            # Check if ALL missing objects are in reverse file\n",
    "            still_missing = missing_textile_ids - found_missing_in_reverse\n",
    "            print(f\"📊 Still missing after reverse check: {len(still_missing)}\")\n",
    "            \n",
    "            if len(still_missing) == 0:\n",
    "                print(f\"🎉 ALL MISSING OBJECTS FOUND IN REVERSE FILE! 🎉\")\n",
    "            else:\n",
    "                print(f\"📋 Still missing: {sorted(list(still_missing))[:10]}\")\n",
    "        else:\n",
    "            print(f\"❌ No missing objects found in reverse file\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading reverse file: {e}\")\n",
    "        return\n",
    "    \n",
    "    # 3. Double-check by loading ALL files and recounting\n",
    "    print(f\"\\n🔍 === DOUBLE-CHECK: RECOUNT WITH ALL FILES INCLUDING REVERSE ===\")\n",
    "    \n",
    "    # Load expected textile IDs\n",
    "    with open(\"textile_object_ids.json\", \"r\") as f:\n",
    "        all_expected_ids = set(json.load(f))\n",
    "    \n",
    "    print(f\"📊 Total expected textile objects: {len(all_expected_ids):,}\")\n",
    "    \n",
    "    # Load ALL textile files INCLUDING reverse\n",
    "    all_files = [\n",
    "        'met_textiles_batch_22800_20250705_134702.json',\n",
    "        'idun/met_textiles_batch_11988_20250705_134921.json', \n",
    "        'remaining_textiles_complete_20250705_135732.json',\n",
    "        'idun/met_textiles_complete_reverse_20250705_222820.json'\n",
    "    ]\n",
    "    \n",
    "    all_downloaded_ids = set()\n",
    "    \n",
    "    for file in all_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            file_ids = set()\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    file_ids.add(obj['objectID'])\n",
    "            \n",
    "            new_ids = file_ids - all_downloaded_ids\n",
    "            all_downloaded_ids.update(file_ids)\n",
    "            \n",
    "            print(f\"📁 {file}: {len(file_ids):,} total, {len(new_ids):,} new\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    # Final count\n",
    "    truly_missing = all_expected_ids - all_downloaded_ids\n",
    "    \n",
    "    print(f\"\\n📊 FINAL RECOUNT RESULTS:\")\n",
    "    print(f\"  Expected: {len(all_expected_ids):,}\")\n",
    "    print(f\"  Downloaded: {len(all_downloaded_ids):,}\")\n",
    "    print(f\"  Actually Missing: {len(truly_missing):,}\")\n",
    "    print(f\"  Success Rate: {len(all_downloaded_ids)/len(all_expected_ids)*100:.3f}%\")\n",
    "    \n",
    "    if len(truly_missing) == 0:\n",
    "        print(f\"\\n🎉🎉🎉 PERFECT! ALL TEXTILE OBJECTS FOUND! 🎉🎉🎉\")\n",
    "    elif len(truly_missing) < 10:\n",
    "        print(f\"\\n✅ Nearly perfect! Only {len(truly_missing)} truly missing:\")\n",
    "        print(f\"📋 Truly missing IDs: {sorted(list(truly_missing))}\")\n",
    "    else:\n",
    "        print(f\"\\n📋 Truly missing IDs (first 20): {sorted(list(truly_missing))[:20]}\")\n",
    "    \n",
    "    # Save corrected analysis\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    corrected_analysis = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"original_missing_count\": len(missing_textile_ids),\n",
    "        \"found_in_reverse_file\": len(found_missing_in_reverse) if 'found_missing_in_reverse' in locals() else 0,\n",
    "        \"truly_missing_count\": len(truly_missing),\n",
    "        \"truly_missing_ids\": sorted(list(truly_missing)),\n",
    "        \"success_rate_percent\": len(all_downloaded_ids)/len(all_expected_ids)*100,\n",
    "        \"total_downloaded\": len(all_downloaded_ids),\n",
    "        \"total_expected\": len(all_expected_ids)\n",
    "    }\n",
    "    \n",
    "    filename = f\"corrected_missing_analysis_{timestamp}.json\"\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(corrected_analysis, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n📁 Corrected analysis saved to: {filename}\")\n",
    "    return corrected_analysis\n",
    "\n",
    "# Run the check\n",
    "corrected_analysis = check_missing_in_reverse_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "070affe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINAL RETRY FOR THE 243 TRULY MISSING OBJECTS ===\n",
      "🎯 Attempting final retry for 243 objects...\n",
      "Progress: 1/243\n",
      "Progress: 11/243\n",
      "Progress: 21/243\n",
      "Progress: 31/243\n",
      "Rate limited on 212246, waiting 10s...\n",
      "Rate limited on 212246, waiting 20s...\n",
      "Progress: 41/243\n",
      "Progress: 51/243\n",
      "Rate limited on 239289, waiting 10s...\n",
      "Rate limited on 239289, waiting 20s...\n",
      "Progress: 61/243\n",
      "Progress: 71/243\n",
      "Progress: 81/243\n",
      "Rate limited on 321230, waiting 10s...\n",
      "Rate limited on 321230, waiting 20s...\n",
      "Progress: 91/243\n",
      "Progress: 101/243\n",
      "Progress: 111/243\n",
      "Rate limited on 479039, waiting 10s...\n",
      "Rate limited on 479039, waiting 20s...\n",
      "Progress: 121/243\n",
      "Progress: 131/243\n",
      "Progress: 141/243\n",
      "Rate limited on 648243, waiting 10s...\n",
      "Rate limited on 648243, waiting 20s...\n",
      "Progress: 151/243\n",
      "Progress: 161/243\n",
      "Progress: 171/243\n",
      "Rate limited on 722213, waiting 10s...\n",
      "Rate limited on 722213, waiting 20s...\n",
      "Progress: 181/243\n",
      "Progress: 191/243\n",
      "Progress: 201/243\n",
      "Rate limited on 760055, waiting 10s...\n",
      "Rate limited on 760055, waiting 20s...\n",
      "Progress: 211/243\n",
      "Progress: 221/243\n",
      "Progress: 231/243\n",
      "Rate limited on 846509, waiting 10s...\n",
      "Rate limited on 846509, waiting 20s...\n",
      "Progress: 241/243\n",
      "\n",
      "🏁 FINAL RESULTS:\n",
      "✅ Recovered: 0\n",
      "❌ Confirmed 404: 243\n",
      "💥 Still failed: 0\n",
      "🎯 NEW SUCCESS RATE: 99.273%\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "def final_retry_missing_243():\n",
    "    print(\"=== FINAL RETRY FOR THE 243 TRULY MISSING OBJECTS ===\")\n",
    "    \n",
    "    # Load the truly missing IDs\n",
    "    with open('corrected_missing_analysis_20250705_223738.json', 'r') as f:\n",
    "        analysis = json.load(f)\n",
    "    missing_ids = analysis['truly_missing_ids']\n",
    "    \n",
    "    print(f\"🎯 Attempting final retry for {len(missing_ids)} objects...\")\n",
    "    \n",
    "    def get_object_with_extreme_patience(object_id):\n",
    "        \"\"\"Try with maximum retries and patience\"\"\"\n",
    "        url = f\"https://collectionapi.metmuseum.org/public/collection/v1/objects/{object_id}\"\n",
    "        \n",
    "        for attempt in range(15):  # More attempts\n",
    "            try:\n",
    "                response = requests.get(url, timeout=60)  # Longer timeout\n",
    "                if response.status_code == 200:\n",
    "                    return response.json(), \"success\"\n",
    "                elif response.status_code == 404:\n",
    "                    return None, \"not_found\"\n",
    "                elif response.status_code == 403:\n",
    "                    wait_time = min(60, 10 * (attempt + 1))\n",
    "                    print(f\"Rate limited on {object_id}, waiting {wait_time}s...\")\n",
    "                    time.sleep(wait_time)\n",
    "                    continue\n",
    "                else:\n",
    "                    time.sleep(5)\n",
    "                    continue\n",
    "            except Exception as e:\n",
    "                print(f\"Error on {object_id}: {e}\")\n",
    "                time.sleep(10)\n",
    "        \n",
    "        return None, \"failed\"\n",
    "    \n",
    "    recovered = []\n",
    "    still_404 = []\n",
    "    still_failed = []\n",
    "    \n",
    "    for i, obj_id in enumerate(missing_ids):\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Progress: {i+1}/{len(missing_ids)}\")\n",
    "        \n",
    "        obj_data, status = get_object_with_extreme_patience(obj_id)\n",
    "        \n",
    "        if status == \"success\":\n",
    "            recovered.append(obj_data)\n",
    "            print(f\"🎉 RECOVERED: {obj_id}\")\n",
    "        elif status == \"not_found\":\n",
    "            still_404.append(obj_id)\n",
    "        else:\n",
    "            still_failed.append(obj_id)\n",
    "        \n",
    "        time.sleep(1)  # Very slow to be respectful\n",
    "    \n",
    "    # Save results\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    if recovered:\n",
    "        with open(f\"final_recovery_{timestamp}.json\", 'w') as f:\n",
    "            json.dump(recovered, f, indent=2)\n",
    "    \n",
    "    final_results = {\n",
    "        \"timestamp\": timestamp,\n",
    "        \"attempted\": len(missing_ids),\n",
    "        \"recovered\": len(recovered),\n",
    "        \"confirmed_404\": len(still_404),\n",
    "        \"still_failed\": len(still_failed),\n",
    "        \"final_success_rate\": (33194 + len(recovered)) / 33437 * 100,\n",
    "        \"confirmed_404_ids\": still_404,\n",
    "        \"still_failed_ids\": still_failed\n",
    "    }\n",
    "    \n",
    "    with open(f\"final_retry_results_{timestamp}.json\", 'w') as f:\n",
    "        json.dump(final_results, f, indent=2)\n",
    "    \n",
    "    print(f\"\\n🏁 FINAL RESULTS:\")\n",
    "    print(f\"✅ Recovered: {len(recovered)}\")\n",
    "    print(f\"❌ Confirmed 404: {len(still_404)}\")\n",
    "    print(f\"💥 Still failed: {len(still_failed)}\")\n",
    "    print(f\"🎯 NEW SUCCESS RATE: {(33194 + len(recovered)) / 33437 * 100:.3f}%\")\n",
    "    \n",
    "    if len(still_404) + len(still_failed) == 0:\n",
    "        print(\"🎉🎉🎉 PERFECT COMPLETION! 🎉🎉🎉\")\n",
    "\n",
    "# Run final retry\n",
    "final_retry_missing_243()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "827002af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== FINALIZING MET TEXTILES DATASET ===\n",
      "Started at: 2025-07-05 22:56:31.169366\n",
      "\n",
      "📁 === STEP 1: CREATING CLEAN DIRECTORY ===\n",
      "✅ Created directory: FINAL_MET_TEXTILES_DATASET\n",
      "✅ Created subdirectory: objects\n",
      "✅ Created subdirectory: lists\n",
      "✅ Created subdirectory: metadata\n",
      "\n",
      "🧵 === STEP 2: COLLECTING ALL TEXTILES DATA ===\n",
      "  📁 met_textiles_batch_22800_20250705_134702.json: 22104 unique objects added\n",
      "  📁 idun/met_textiles_batch_11988_20250705_134921.json: 10354 unique objects added\n",
      "  📁 remaining_textiles_complete_20250705_135732.json: 736 unique objects added\n",
      "  📁 idun/met_textiles_complete_reverse_20250705_222820.json: 0 unique objects added\n",
      "✅ Total unique textiles collected: 33,194\n",
      "✅ Textiles with images: 27,271\n",
      "\n",
      "🎨 === STEP 3: COLLECTING ALL TAPESTRIES DATA ===\n",
      "  📁 missing_tapestries_complete_20250705_191910.json: 150 unique objects added\n",
      "✅ Total unique tapestries collected: 150\n",
      "✅ Tapestries with images: 102\n",
      "\n",
      "🔗 === STEP 4: ANALYZING INTERSECTIONS ===\n",
      "✅ Objects in both textiles and tapestries: 0\n",
      "\n",
      "❌ === STEP 5: COLLECTING FAILED OBJECTS ===\n",
      "  ❌ Error loading final_retry_results_20250705_224324.json: [Errno 2] No such file or directory: 'final_retry_results_20250705_224324.json'\n",
      "  📁 not_found_ids_20250705_135732.json: processed\n",
      "  📁 failed_ids_20250705_135732.json: processed\n",
      "✅ Total failed object IDs: 243\n",
      "✅ Confirmed 404 IDs: 0\n",
      "\n",
      "💾 === STEP 6: SAVING FINAL FILES ===\n",
      "  ✅ FINAL_MET_TEXTILES_DATASET/objects/complete_textiles_20250705_225632.json: 33,194 objects\n",
      "  ✅ FINAL_MET_TEXTILES_DATASET/objects/complete_tapestries_20250705_225632.json: 150 objects\n",
      "  ✅ FINAL_MET_TEXTILES_DATASET/objects/textiles_tapestries_intersection_20250705_225632.json: 0 objects\n",
      "  ✅ FINAL_MET_TEXTILES_DATASET/objects/textiles_with_images_20250705_225632.json: 27,271 objects\n",
      "  ✅ FINAL_MET_TEXTILES_DATASET/objects/tapestries_with_images_20250705_225632.json: 102 objects\n",
      "  ✅ FINAL_MET_TEXTILES_DATASET/lists/textile_object_ids_20250705_225632.json: 33,194 IDs\n",
      "  ✅ FINAL_MET_TEXTILES_DATASET/lists/tapestry_object_ids_20250705_225632.json: 150 IDs\n",
      "  ✅ FINAL_MET_TEXTILES_DATASET/lists/intersection_object_ids_20250705_225632.json: 0 IDs\n",
      "  ✅ FINAL_MET_TEXTILES_DATASET/lists/failed_object_ids_20250705_225632.json: 243 IDs\n",
      "  ✅ FINAL_MET_TEXTILES_DATASET/lists/confirmed_404_ids_20250705_225632.json: 0 IDs\n",
      "\n",
      "📋 === STEP 7: CREATING DATASET METADATA ===\n",
      "  ✅ FINAL_MET_TEXTILES_DATASET/metadata/dataset_metadata_20250705_225632.json: Complete dataset metadata\n",
      "  ✅ FINAL_MET_TEXTILES_DATASET/README.md: Documentation created\n",
      "\n",
      "🎉 === DATASET FINALIZATION COMPLETE ===\n",
      "📁 Final directory: FINAL_MET_TEXTILES_DATASET\n",
      "📊 Total files created: 12\n",
      "🧵 Textiles: 33,194 objects\n",
      "🎨 Tapestries: 150 objects\n",
      "🔗 Intersections: 0 objects\n",
      "🖼️  With Images: 27,373 objects\n",
      "❌ Failed: 243 objects\n",
      "✅ Success Rate: 99.273%\n",
      "\n",
      "🎯 Your MET Textiles dataset is ready for research! 🎯\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def finalize_dataset():\n",
    "    print(\"=== FINALIZING MET TEXTILES DATASET ===\")\n",
    "    print(f\"Started at: {datetime.now()}\")\n",
    "    \n",
    "    # 1. CREATE NEW DIRECTORY FOR FINAL FILES\n",
    "    print(\"\\n📁 === STEP 1: CREATING CLEAN DIRECTORY ===\")\n",
    "    \n",
    "    final_dir = \"FINAL_MET_TEXTILES_DATASET\"\n",
    "    if os.path.exists(final_dir):\n",
    "        shutil.rmtree(final_dir)\n",
    "    os.makedirs(final_dir)\n",
    "    print(f\"✅ Created directory: {final_dir}\")\n",
    "    \n",
    "    # Create subdirectories\n",
    "    subdirs = [\"objects\", \"lists\", \"metadata\"]\n",
    "    for subdir in subdirs:\n",
    "        os.makedirs(os.path.join(final_dir, subdir))\n",
    "        print(f\"✅ Created subdirectory: {subdir}\")\n",
    "    \n",
    "    # 2. COLLECT ALL TEXTILES DATA\n",
    "    print(\"\\n🧵 === STEP 2: COLLECTING ALL TEXTILES DATA ===\")\n",
    "    \n",
    "    textile_files = [\n",
    "        'met_textiles_batch_22800_20250705_134702.json',\n",
    "        'idun/met_textiles_batch_11988_20250705_134921.json',\n",
    "        'remaining_textiles_complete_20250705_135732.json',\n",
    "        'idun/met_textiles_complete_reverse_20250705_222820.json'\n",
    "    ]\n",
    "    \n",
    "    all_textiles = []\n",
    "    all_textile_ids = set()\n",
    "    textiles_with_images = []\n",
    "    \n",
    "    for file in textile_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            file_count = 0\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    if obj['objectID'] not in all_textile_ids:\n",
    "                        all_textiles.append(obj)\n",
    "                        all_textile_ids.add(obj['objectID'])\n",
    "                        file_count += 1\n",
    "                        \n",
    "                        if obj.get('primaryImage'):\n",
    "                            textiles_with_images.append(obj)\n",
    "            \n",
    "            print(f\"  📁 {file}: {file_count} unique objects added\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    print(f\"✅ Total unique textiles collected: {len(all_textiles):,}\")\n",
    "    print(f\"✅ Textiles with images: {len(textiles_with_images):,}\")\n",
    "    \n",
    "    # 3. COLLECT ALL TAPESTRIES DATA\n",
    "    print(\"\\n🎨 === STEP 3: COLLECTING ALL TAPESTRIES DATA ===\")\n",
    "    \n",
    "    all_tapestries = []\n",
    "    all_tapestry_ids = set()\n",
    "    tapestries_with_images = []\n",
    "    \n",
    "    tapestry_files = [f for f in os.listdir('.') if f.startswith('missing_tapestries_complete_')]\n",
    "    \n",
    "    for file in tapestry_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            file_count = 0\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    if obj['objectID'] not in all_tapestry_ids:\n",
    "                        all_tapestries.append(obj)\n",
    "                        all_tapestry_ids.add(obj['objectID'])\n",
    "                        file_count += 1\n",
    "                        \n",
    "                        if obj.get('primaryImage'):\n",
    "                            tapestries_with_images.append(obj)\n",
    "            \n",
    "            print(f\"  📁 {file}: {file_count} unique objects added\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    print(f\"✅ Total unique tapestries collected: {len(all_tapestries):,}\")\n",
    "    print(f\"✅ Tapestries with images: {len(tapestries_with_images):,}\")\n",
    "    \n",
    "    # 4. FIND INTERSECTIONS\n",
    "    print(\"\\n🔗 === STEP 4: ANALYZING INTERSECTIONS ===\")\n",
    "    \n",
    "    # Find objects that appear in both textiles and tapestries\n",
    "    intersection_ids = all_textile_ids & all_tapestry_ids\n",
    "    intersection_objects = [obj for obj in all_textiles if obj['objectID'] in intersection_ids]\n",
    "    \n",
    "    print(f\"✅ Objects in both textiles and tapestries: {len(intersection_ids):,}\")\n",
    "    \n",
    "    # 5. COLLECT FAILED IDs\n",
    "    print(\"\\n❌ === STEP 5: COLLECTING FAILED OBJECTS ===\")\n",
    "    \n",
    "    failed_files = [\n",
    "        'final_retry_results_20250705_224324.json',  # Latest results\n",
    "        'not_found_ids_20250705_135732.json',\n",
    "        'failed_ids_20250705_135732.json'\n",
    "    ]\n",
    "    \n",
    "    all_failed_ids = set()\n",
    "    confirmed_404_ids = set()\n",
    "    \n",
    "    for file in failed_files:\n",
    "        try:\n",
    "            with open(file, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            # Handle different file formats\n",
    "            if 'confirmed_404_ids' in data:\n",
    "                confirmed_404_ids.update(data['confirmed_404_ids'])\n",
    "                all_failed_ids.update(data['confirmed_404_ids'])\n",
    "                if 'still_failed_ids' in data:\n",
    "                    all_failed_ids.update(data['still_failed_ids'])\n",
    "            else:\n",
    "                # Regular list of IDs\n",
    "                all_failed_ids.update(data)\n",
    "            \n",
    "            print(f\"  📁 {file}: processed\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    print(f\"✅ Total failed object IDs: {len(all_failed_ids):,}\")\n",
    "    print(f\"✅ Confirmed 404 IDs: {len(confirmed_404_ids):,}\")\n",
    "    \n",
    "    # 6. SAVE ALL FINAL FILES\n",
    "    print(\"\\n💾 === STEP 6: SAVING FINAL FILES ===\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # Save complete objects collections\n",
    "    files_to_save = [\n",
    "        {\n",
    "            \"data\": all_textiles,\n",
    "            \"filename\": f\"{final_dir}/objects/complete_textiles_{timestamp}.json\",\n",
    "            \"description\": \"All textile objects with full metadata\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": all_tapestries,\n",
    "            \"filename\": f\"{final_dir}/objects/complete_tapestries_{timestamp}.json\",\n",
    "            \"description\": \"All tapestry objects with full metadata\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": intersection_objects,\n",
    "            \"filename\": f\"{final_dir}/objects/textiles_tapestries_intersection_{timestamp}.json\",\n",
    "            \"description\": \"Objects that are both textiles and tapestries\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": textiles_with_images,\n",
    "            \"filename\": f\"{final_dir}/objects/textiles_with_images_{timestamp}.json\",\n",
    "            \"description\": \"Textile objects that have primary images\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": tapestries_with_images,\n",
    "            \"filename\": f\"{final_dir}/objects/tapestries_with_images_{timestamp}.json\",\n",
    "            \"description\": \"Tapestry objects that have primary images\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for file_info in files_to_save:\n",
    "        with open(file_info[\"filename\"], 'w', encoding='utf-8') as f:\n",
    "            json.dump(file_info[\"data\"], f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  ✅ {file_info['filename']}: {len(file_info['data']):,} objects\")\n",
    "    \n",
    "    # Save ID lists only\n",
    "    id_lists = [\n",
    "        {\n",
    "            \"data\": sorted(list(all_textile_ids)),\n",
    "            \"filename\": f\"{final_dir}/lists/textile_object_ids_{timestamp}.json\",\n",
    "            \"description\": \"List of all textile object IDs\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": sorted(list(all_tapestry_ids)),\n",
    "            \"filename\": f\"{final_dir}/lists/tapestry_object_ids_{timestamp}.json\",\n",
    "            \"description\": \"List of all tapestry object IDs\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": sorted(list(intersection_ids)),\n",
    "            \"filename\": f\"{final_dir}/lists/intersection_object_ids_{timestamp}.json\",\n",
    "            \"description\": \"List of object IDs that are both textiles and tapestries\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": sorted(list(all_failed_ids)),\n",
    "            \"filename\": f\"{final_dir}/lists/failed_object_ids_{timestamp}.json\",\n",
    "            \"description\": \"List of object IDs that failed to download\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": sorted(list(confirmed_404_ids)),\n",
    "            \"filename\": f\"{final_dir}/lists/confirmed_404_ids_{timestamp}.json\",\n",
    "            \"description\": \"List of object IDs confirmed as not found (404)\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for id_list in id_lists:\n",
    "        with open(id_list[\"filename\"], 'w') as f:\n",
    "            json.dump(id_list[\"data\"], f, indent=2)\n",
    "        print(f\"  ✅ {id_list['filename']}: {len(id_list['data']):,} IDs\")\n",
    "    \n",
    "    # 7. CREATE COMPREHENSIVE METADATA\n",
    "    print(\"\\n📋 === STEP 7: CREATING DATASET METADATA ===\")\n",
    "    \n",
    "    # Load original expected counts\n",
    "    with open(\"textile_object_ids.json\", \"r\") as f:\n",
    "        original_textile_ids = json.load(f)\n",
    "    \n",
    "    dataset_metadata = {\n",
    "        \"dataset_info\": {\n",
    "            \"name\": \"MET Museum Textiles and Tapestries Collection\",\n",
    "            \"version\": \"1.0\",\n",
    "            \"created_date\": timestamp,\n",
    "            \"description\": \"Complete collection of textile and tapestry objects from the Metropolitan Museum of Art\",\n",
    "            \"success_rate_percent\": 99.273,\n",
    "            \"total_api_calls_estimated\": 35000,\n",
    "            \"download_duration_hours\": 8\n",
    "        },\n",
    "        \"collection_statistics\": {\n",
    "            \"textiles\": {\n",
    "                \"total_expected\": len(original_textile_ids),\n",
    "                \"successfully_downloaded\": len(all_textiles),\n",
    "                \"with_images\": len(textiles_with_images),\n",
    "                \"success_rate\": len(all_textiles) / len(original_textile_ids) * 100\n",
    "            },\n",
    "            \"tapestries\": {\n",
    "                \"total_downloaded\": len(all_tapestries),\n",
    "                \"with_images\": len(tapestries_with_images),\n",
    "                \"unique_to_tapestries\": len(all_tapestry_ids - all_textile_ids)\n",
    "            },\n",
    "            \"intersections\": {\n",
    "                \"objects_in_both_categories\": len(intersection_ids),\n",
    "                \"percentage_of_textiles\": len(intersection_ids) / len(all_textiles) * 100 if all_textiles else 0\n",
    "            },\n",
    "            \"failures\": {\n",
    "                \"total_failed\": len(all_failed_ids),\n",
    "                \"confirmed_404\": len(confirmed_404_ids),\n",
    "                \"failure_rate\": len(all_failed_ids) / len(original_textile_ids) * 100\n",
    "            }\n",
    "        },\n",
    "        \"file_descriptions\": {\n",
    "            \"objects/\": \"Complete object data with full metadata\",\n",
    "            \"lists/\": \"Object ID lists for various categories\",\n",
    "            \"metadata/\": \"Dataset documentation and statistics\"\n",
    "        },\n",
    "        \"usage_notes\": [\n",
    "            \"All objects include full MET API metadata\",\n",
    "            \"Objects with 'primaryImage' field have downloadable images\",\n",
    "            \"Intersection objects appear in both textile and tapestry searches\",\n",
    "            \"Failed IDs are mostly confirmed 404 (object no longer exists)\",\n",
    "            \"Use object IDs to fetch latest data from MET API if needed\"\n",
    "        ]\n",
    "    }\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata_file = f\"{final_dir}/metadata/dataset_metadata_{timestamp}.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(dataset_metadata, f, indent=2)\n",
    "    print(f\"  ✅ {metadata_file}: Complete dataset metadata\")\n",
    "    \n",
    "    # Create README\n",
    "    readme_content = f\"\"\"# MET Museum Textiles and Tapestries Dataset\n",
    "\n",
    "## Overview\n",
    "This dataset contains {len(all_textiles):,} textile objects and {len(all_tapestries):,} tapestry objects from the Metropolitan Museum of Art, collected via their public API.\n",
    "\n",
    "## Success Rate: {99.273:.3f}%\n",
    "\n",
    "## Directory Structure\n",
    "```\n",
    "FINAL_MET_TEXTILES_DATASET/\n",
    "├── objects/          # Complete object data with metadata\n",
    "├── lists/            # Object ID lists\n",
    "├── metadata/         # Dataset documentation\n",
    "└── README.md         # This file\n",
    "```\n",
    "\n",
    "## Statistics\n",
    "- **Textiles**: {len(all_textiles):,} objects ({len(textiles_with_images):,} with images)\n",
    "- **Tapestries**: {len(all_tapestries):,} objects ({len(tapestries_with_images):,} with images)\n",
    "- **Intersections**: {len(intersection_ids):,} objects appear in both categories\n",
    "- **Failed Downloads**: {len(all_failed_ids):,} objects (mostly confirmed 404s)\n",
    "\n",
    "## Files Created: {timestamp}\n",
    "\n",
    "### Object Files (Complete Metadata)\n",
    "- `complete_textiles_{timestamp}.json` - All textile objects\n",
    "- `complete_tapestries_{timestamp}.json` - All tapestry objects  \n",
    "- `textiles_tapestries_intersection_{timestamp}.json` - Objects in both categories\n",
    "- `textiles_with_images_{timestamp}.json` - Textiles with images\n",
    "- `tapestries_with_images_{timestamp}.json` - Tapestries with images\n",
    "\n",
    "### ID Lists (Object IDs Only)\n",
    "- `textile_object_ids_{timestamp}.json` - All textile IDs\n",
    "- `tapestry_object_ids_{timestamp}.json` - All tapestry IDs\n",
    "- `intersection_object_ids_{timestamp}.json` - Intersection IDs\n",
    "- `failed_object_ids_{timestamp}.json` - Failed download IDs\n",
    "- `confirmed_404_ids_{timestamp}.json` - Confirmed non-existent IDs\n",
    "\n",
    "## Usage\n",
    "Each object contains full MET API metadata including:\n",
    "- Object details (title, artist, date, medium, etc.)\n",
    "- Image URLs (if available)\n",
    "- Department and classification info\n",
    "- Measurement and location data\n",
    "\n",
    "## Data Quality\n",
    "- {99.273:.3f}% success rate\n",
    "- All duplicates removed\n",
    "- Failed objects documented\n",
    "- Complete provenance tracking\n",
    "\n",
    "Generated: {datetime.now()}\n",
    "\"\"\"\n",
    "    \n",
    "    readme_file = f\"{final_dir}/README.md\"\n",
    "    with open(readme_file, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    print(f\"  ✅ {readme_file}: Documentation created\")\n",
    "    \n",
    "    # 8. FINAL SUMMARY\n",
    "    print(f\"\\n🎉 === DATASET FINALIZATION COMPLETE ===\")\n",
    "    print(f\"📁 Final directory: {final_dir}\")\n",
    "    print(f\"📊 Total files created: {len(files_to_save) + len(id_lists) + 2}\")  # +2 for metadata and readme\n",
    "    print(f\"🧵 Textiles: {len(all_textiles):,} objects\")\n",
    "    print(f\"🎨 Tapestries: {len(all_tapestries):,} objects\")\n",
    "    print(f\"🔗 Intersections: {len(intersection_ids):,} objects\")\n",
    "    print(f\"🖼️  With Images: {len(textiles_with_images) + len(tapestries_with_images):,} objects\")\n",
    "    print(f\"❌ Failed: {len(all_failed_ids):,} objects\")\n",
    "    print(f\"✅ Success Rate: {99.273:.3f}%\")\n",
    "    print(f\"\\n🎯 Your MET Textiles dataset is ready for research! 🎯\")\n",
    "    \n",
    "    return {\n",
    "        \"final_directory\": final_dir,\n",
    "        \"textiles_count\": len(all_textiles),\n",
    "        \"tapestries_count\": len(all_tapestries),\n",
    "        \"intersections_count\": len(intersection_ids),\n",
    "        \"with_images_count\": len(textiles_with_images) + len(tapestries_with_images),\n",
    "        \"failed_count\": len(all_failed_ids),\n",
    "        \"success_rate\": 99.273\n",
    "    }\n",
    "\n",
    "# Run the finalization\n",
    "final_stats = finalize_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bf68f86f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== CREATING CORRECTED FINAL MET TEXTILES DATASET ===\n",
      "Started at: 2025-07-05 23:03:14.558119\n",
      "\n",
      "📁 === STEP 1: CREATING CLEAN DIRECTORY ===\n",
      "✅ Created directory: FINAL_CORRECTED_MET_TEXTILES_DATASET\n",
      "✅ Created subdirectory: all_objects\n",
      "✅ Created subdirectory: objects_with_images_only\n",
      "✅ Created subdirectory: id_lists\n",
      "✅ Created subdirectory: metadata\n",
      "\n",
      "📊 === STEP 2: GETTING EXPECTED COUNTS FROM API ===\n",
      "📊 Expected Textiles from API: 33,437\n",
      "📊 Expected Tapestries (unique): 151\n",
      "\n",
      "🧵 === STEP 3: COLLECTING ALL DOWNLOADED TEXTILES ===\n",
      "  📁 met_textiles_batch_22800_20250705_134702.json: 22104 unique objects added\n",
      "  📁 idun/met_textiles_batch_11988_20250705_134921.json: 10354 unique objects added\n",
      "  📁 remaining_textiles_complete_20250705_135732.json: 736 unique objects added\n",
      "  📁 idun/met_textiles_complete_reverse_20250705_222820.json: 0 unique objects added\n",
      "✅ Downloaded Textiles: 33,194 out of 33,437\n",
      "✅ Textiles with images: 27,271\n",
      "❌ Failed Textiles: 243\n",
      "\n",
      "🎨 === STEP 4: COLLECTING ALL DOWNLOADED TAPESTRIES ===\n",
      "  📁 missing_tapestries_complete_20250705_191910.json: 150 unique objects added\n",
      "✅ Downloaded Tapestries: 150 out of 151\n",
      "✅ Tapestries with images: 102\n",
      "❌ Failed Tapestries: 1\n",
      "\n",
      "🔗 === STEP 5: ANALYZING INTERSECTIONS ===\n",
      "✅ Objects in both textiles and tapestries: 0\n",
      "📊 Tapestries already in textiles: 0\n",
      "\n",
      "📊 === STEP 6: CALCULATING FINAL TOTALS ===\n",
      "📊 FINAL TOTALS:\n",
      "  Expected Total: 33,588\n",
      "  Downloaded Total: 33,344\n",
      "  Total with Images: 27,373\n",
      "  Total Failed: 244\n",
      "  Success Rate: 99.274%\n",
      "\n",
      "🖼️  === STEP 7: CREATING OBJECTS WITH IMAGES COLLECTION ===\n",
      "✅ Total objects with images: 27,373\n",
      "✅ This will be your FINAL RESEARCH DATASET!\n",
      "\n",
      "💾 === STEP 8: SAVING ALL FILES ===\n",
      "  📁 FINAL_CORRECTED_MET_TEXTILES_DATASET/all_objects/complete_textiles_20250705_230315.json: 33,194 objects\n",
      "  📁 FINAL_CORRECTED_MET_TEXTILES_DATASET/all_objects/complete_tapestries_20250705_230315.json: 150 objects\n",
      "  📁 FINAL_CORRECTED_MET_TEXTILES_DATASET/all_objects/textiles_tapestries_intersection_20250705_230315.json: 0 objects\n",
      "  🖼️  FINAL_CORRECTED_MET_TEXTILES_DATASET/objects_with_images_only/textiles_with_images_20250705_230315.json: 27,271 objects\n",
      "  🖼️  FINAL_CORRECTED_MET_TEXTILES_DATASET/objects_with_images_only/tapestries_with_images_20250705_230315.json: 102 objects\n",
      "  🖼️  FINAL_CORRECTED_MET_TEXTILES_DATASET/objects_with_images_only/ALL_TEXTILES_AND_TAPESTRIES_WITH_IMAGES_20250705_230315.json: 27,373 objects\n",
      "  ⚠️  Could not load failed IDs: [Errno 2] No such file or directory: 'final_retry_results_20250705_224324.json'\n",
      "  📋 FINAL_CORRECTED_MET_TEXTILES_DATASET/id_lists/textile_object_ids_20250705_230315.json: 33,194 IDs\n",
      "  📋 FINAL_CORRECTED_MET_TEXTILES_DATASET/id_lists/tapestry_object_ids_20250705_230315.json: 150 IDs\n",
      "  📋 FINAL_CORRECTED_MET_TEXTILES_DATASET/id_lists/all_objects_with_images_ids_20250705_230315.json: 27,373 IDs\n",
      "\n",
      "📋 === STEP 9: CREATING DATASET METADATA ===\n",
      "  📊 FINAL_CORRECTED_MET_TEXTILES_DATASET/metadata/dataset_metadata_20250705_230315.json: Complete metadata saved\n",
      "  📝 FINAL_CORRECTED_MET_TEXTILES_DATASET/README.md: Documentation created\n",
      "\n",
      "🎉🎉🎉 === CORRECTED DATASET FINALIZATION COMPLETE === 🎉🎉🎉\n",
      "📁 Final directory: FINAL_CORRECTED_MET_TEXTILES_DATASET\n",
      "📊 API Expected: 33,588 objects\n",
      "✅ Successfully Downloaded: 33,344 objects\n",
      "🖼️  Objects with Images: 27,373 objects\n",
      "❌ Failed: 244 objects\n",
      "🎯 Success Rate: 99.274%\n",
      "\n",
      "🏆 YOUR FINAL RESEARCH DATASET: 27,373 OBJECTS WITH IMAGES!\n",
      "📂 Located in: objects_with_images_only/ALL_TEXTILES_AND_TAPESTRIES_WITH_IMAGES_20250705_230315.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "import shutil\n",
    "from datetime import datetime\n",
    "\n",
    "def create_final_corrected_dataset():\n",
    "    print(\"=== CREATING CORRECTED FINAL MET TEXTILES DATASET ===\")\n",
    "    print(f\"Started at: {datetime.now()}\")\n",
    "    \n",
    "    # 1. CREATE NEW DIRECTORY FOR FINAL FILES\n",
    "    print(\"\\n📁 === STEP 1: CREATING CLEAN DIRECTORY ===\")\n",
    "    \n",
    "    final_dir = \"FINAL_CORRECTED_MET_TEXTILES_DATASET\"\n",
    "    if os.path.exists(final_dir):\n",
    "        shutil.rmtree(final_dir)\n",
    "    os.makedirs(final_dir)\n",
    "    print(f\"✅ Created directory: {final_dir}\")\n",
    "    \n",
    "    # Create subdirectories\n",
    "    subdirs = [\"all_objects\", \"objects_with_images_only\", \"id_lists\", \"metadata\"]\n",
    "    for subdir in subdirs:\n",
    "        os.makedirs(os.path.join(final_dir, subdir))\n",
    "        print(f\"✅ Created subdirectory: {subdir}\")\n",
    "    \n",
    "    # 2. GET EXPECTED COUNTS FROM API SEARCH\n",
    "    print(\"\\n📊 === STEP 2: GETTING EXPECTED COUNTS FROM API ===\")\n",
    "    \n",
    "    # Load original expected textile count\n",
    "    with open(\"textile_object_ids.json\", \"r\") as f:\n",
    "        expected_textile_ids = set(json.load(f))\n",
    "    expected_textile_count = len(expected_textile_ids)\n",
    "    print(f\"📊 Expected Textiles from API: {expected_textile_count:,}\")\n",
    "    \n",
    "    # Get expected tapestry count from summary\n",
    "    try:\n",
    "        with open('missing_tapestries_summary_20250705_191910.json', 'r') as f:\n",
    "            tapestry_summary = json.load(f)\n",
    "        expected_tapestry_count = tapestry_summary['analysis']['tapestries_missing_from_textiles']\n",
    "        print(f\"📊 Expected Tapestries (unique): {expected_tapestry_count}\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading tapestry summary: {e}\")\n",
    "        expected_tapestry_count = 0\n",
    "    \n",
    "    # 3. COLLECT ALL DOWNLOADED TEXTILES\n",
    "    print(\"\\n🧵 === STEP 3: COLLECTING ALL DOWNLOADED TEXTILES ===\")\n",
    "    \n",
    "    textile_files = [\n",
    "        'met_textiles_batch_22800_20250705_134702.json',\n",
    "        'idun/met_textiles_batch_11988_20250705_134921.json',\n",
    "        'remaining_textiles_complete_20250705_135732.json',\n",
    "        'idun/met_textiles_complete_reverse_20250705_222820.json'\n",
    "    ]\n",
    "    \n",
    "    all_textiles = []\n",
    "    all_textile_ids = set()\n",
    "    textiles_with_images = []\n",
    "    \n",
    "    for file in textile_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            file_count = 0\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    if obj['objectID'] not in all_textile_ids:\n",
    "                        all_textiles.append(obj)\n",
    "                        all_textile_ids.add(obj['objectID'])\n",
    "                        file_count += 1\n",
    "                        \n",
    "                        if obj.get('primaryImage'):\n",
    "                            textiles_with_images.append(obj)\n",
    "            \n",
    "            print(f\"  📁 {file}: {file_count} unique objects added\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    downloaded_textile_count = len(all_textiles)\n",
    "    textile_images_count = len(textiles_with_images)\n",
    "    textile_failed_count = expected_textile_count - downloaded_textile_count\n",
    "    \n",
    "    print(f\"✅ Downloaded Textiles: {downloaded_textile_count:,} out of {expected_textile_count:,}\")\n",
    "    print(f\"✅ Textiles with images: {textile_images_count:,}\")\n",
    "    print(f\"❌ Failed Textiles: {textile_failed_count}\")\n",
    "    \n",
    "    # 4. COLLECT ALL DOWNLOADED TAPESTRIES\n",
    "    print(\"\\n🎨 === STEP 4: COLLECTING ALL DOWNLOADED TAPESTRIES ===\")\n",
    "    \n",
    "    all_tapestries = []\n",
    "    all_tapestry_ids = set()\n",
    "    tapestries_with_images = []\n",
    "    \n",
    "    tapestry_files = [f for f in os.listdir('.') if f.startswith('missing_tapestries_complete_')]\n",
    "    \n",
    "    for file in tapestry_files:\n",
    "        try:\n",
    "            with open(file, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "            \n",
    "            file_count = 0\n",
    "            for obj in data:\n",
    "                if isinstance(obj, dict) and 'objectID' in obj:\n",
    "                    if obj['objectID'] not in all_tapestry_ids:\n",
    "                        all_tapestries.append(obj)\n",
    "                        all_tapestry_ids.add(obj['objectID'])\n",
    "                        file_count += 1\n",
    "                        \n",
    "                        if obj.get('primaryImage'):\n",
    "                            tapestries_with_images.append(obj)\n",
    "            \n",
    "            print(f\"  📁 {file}: {file_count} unique objects added\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error loading {file}: {e}\")\n",
    "    \n",
    "    downloaded_tapestry_count = len(all_tapestries)\n",
    "    tapestry_images_count = len(tapestries_with_images)\n",
    "    tapestry_failed_count = expected_tapestry_count - downloaded_tapestry_count\n",
    "    \n",
    "    print(f\"✅ Downloaded Tapestries: {downloaded_tapestry_count} out of {expected_tapestry_count}\")\n",
    "    print(f\"✅ Tapestries with images: {tapestry_images_count}\")\n",
    "    print(f\"❌ Failed Tapestries: {tapestry_failed_count}\")\n",
    "    \n",
    "    # 5. FIND INTERSECTIONS\n",
    "    print(\"\\n🔗 === STEP 5: ANALYZING INTERSECTIONS ===\")\n",
    "    \n",
    "    # Check for intersection (should be 0 based on your data)\n",
    "    intersection_ids = all_textile_ids & all_tapestry_ids\n",
    "    intersection_objects = [obj for obj in all_textiles if obj['objectID'] in intersection_ids]\n",
    "    \n",
    "    print(f\"✅ Objects in both textiles and tapestries: {len(intersection_ids)}\")\n",
    "    \n",
    "    # The rest of tapestries are in textile intersection (as you mentioned)\n",
    "    tapestries_in_textiles = expected_tapestry_count - downloaded_tapestry_count - tapestry_failed_count\n",
    "    print(f\"📊 Tapestries already in textiles: {tapestries_in_textiles}\")\n",
    "    \n",
    "    # 6. CALCULATE FINAL TOTALS\n",
    "    print(\"\\n📊 === STEP 6: CALCULATING FINAL TOTALS ===\")\n",
    "    \n",
    "    total_failed = textile_failed_count + tapestry_failed_count\n",
    "    total_downloaded = downloaded_textile_count + downloaded_tapestry_count\n",
    "    total_expected = expected_textile_count + expected_tapestry_count\n",
    "    total_with_images = textile_images_count + tapestry_images_count\n",
    "    \n",
    "    print(f\"📊 FINAL TOTALS:\")\n",
    "    print(f\"  Expected Total: {total_expected:,}\")\n",
    "    print(f\"  Downloaded Total: {total_downloaded:,}\")\n",
    "    print(f\"  Total with Images: {total_with_images:,}\")\n",
    "    print(f\"  Total Failed: {total_failed}\")\n",
    "    print(f\"  Success Rate: {total_downloaded/total_expected*100:.3f}%\")\n",
    "    \n",
    "    # 7. CREATE SPECIAL COLLECTION: ALL OBJECTS WITH IMAGES ONLY\n",
    "    print(\"\\n🖼️  === STEP 7: CREATING OBJECTS WITH IMAGES COLLECTION ===\")\n",
    "    \n",
    "    all_objects_with_images = textiles_with_images + tapestries_with_images\n",
    "    print(f\"✅ Total objects with images: {len(all_objects_with_images):,}\")\n",
    "    print(f\"✅ This will be your FINAL RESEARCH DATASET!\")\n",
    "    \n",
    "    # 8. SAVE ALL FILES\n",
    "    print(\"\\n💾 === STEP 8: SAVING ALL FILES ===\")\n",
    "    \n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    # A. Save complete objects collections\n",
    "    complete_objects = [\n",
    "        {\n",
    "            \"data\": all_textiles,\n",
    "            \"filename\": f\"{final_dir}/all_objects/complete_textiles_{timestamp}.json\",\n",
    "            \"description\": \"All downloaded textile objects with full metadata\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": all_tapestries,\n",
    "            \"filename\": f\"{final_dir}/all_objects/complete_tapestries_{timestamp}.json\",\n",
    "            \"description\": \"All downloaded tapestry objects with full metadata\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": intersection_objects,\n",
    "            \"filename\": f\"{final_dir}/all_objects/textiles_tapestries_intersection_{timestamp}.json\",\n",
    "            \"description\": \"Objects that appear in both categories\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for file_info in complete_objects:\n",
    "        with open(file_info[\"filename\"], 'w', encoding='utf-8') as f:\n",
    "            json.dump(file_info[\"data\"], f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  📁 {file_info['filename']}: {len(file_info['data']):,} objects\")\n",
    "    \n",
    "    # B. Save IMAGES ONLY collections (FINAL RESEARCH DATASET)\n",
    "    images_objects = [\n",
    "        {\n",
    "            \"data\": textiles_with_images,\n",
    "            \"filename\": f\"{final_dir}/objects_with_images_only/textiles_with_images_{timestamp}.json\",\n",
    "            \"description\": \"Textile objects with images - RESEARCH READY\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": tapestries_with_images,\n",
    "            \"filename\": f\"{final_dir}/objects_with_images_only/tapestries_with_images_{timestamp}.json\",\n",
    "            \"description\": \"Tapestry objects with images - RESEARCH READY\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": all_objects_with_images,\n",
    "            \"filename\": f\"{final_dir}/objects_with_images_only/ALL_TEXTILES_AND_TAPESTRIES_WITH_IMAGES_{timestamp}.json\",\n",
    "            \"description\": \"🎯 FINAL RESEARCH DATASET - All objects with images\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    for file_info in images_objects:\n",
    "        with open(file_info[\"filename\"], 'w', encoding='utf-8') as f:\n",
    "            json.dump(file_info[\"data\"], f, indent=2, ensure_ascii=False)\n",
    "        print(f\"  🖼️  {file_info['filename']}: {len(file_info['data']):,} objects\")\n",
    "    \n",
    "    # C. Save ID lists\n",
    "    id_lists = [\n",
    "        {\n",
    "            \"data\": sorted(list(all_textile_ids)),\n",
    "            \"filename\": f\"{final_dir}/id_lists/textile_object_ids_{timestamp}.json\",\n",
    "            \"description\": \"All downloaded textile object IDs\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": sorted(list(all_tapestry_ids)),\n",
    "            \"filename\": f\"{final_dir}/id_lists/tapestry_object_ids_{timestamp}.json\",\n",
    "            \"description\": \"All downloaded tapestry object IDs\"\n",
    "        },\n",
    "        {\n",
    "            \"data\": [obj['objectID'] for obj in all_objects_with_images],\n",
    "            \"filename\": f\"{final_dir}/id_lists/all_objects_with_images_ids_{timestamp}.json\",\n",
    "            \"description\": \"IDs of all objects with images (FINAL DATASET)\"\n",
    "        }\n",
    "    ]\n",
    "    \n",
    "    # Load failed IDs\n",
    "    try:\n",
    "        with open('final_retry_results_20250705_224324.json', 'r') as f:\n",
    "            failed_data = json.load(f)\n",
    "        all_failed_ids = failed_data.get('confirmed_404_ids', [])\n",
    "        \n",
    "        id_lists.append({\n",
    "            \"data\": sorted(all_failed_ids),\n",
    "            \"filename\": f\"{final_dir}/id_lists/failed_object_ids_{timestamp}.json\",\n",
    "            \"description\": f\"Failed object IDs ({len(all_failed_ids)} total)\"\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"  ⚠️  Could not load failed IDs: {e}\")\n",
    "    \n",
    "    for id_list in id_lists:\n",
    "        with open(id_list[\"filename\"], 'w') as f:\n",
    "            json.dump(id_list[\"data\"], f, indent=2)\n",
    "        print(f\"  📋 {id_list['filename']}: {len(id_list['data']):,} IDs\")\n",
    "    \n",
    "    # 9. CREATE COMPREHENSIVE METADATA\n",
    "    print(\"\\n📋 === STEP 9: CREATING DATASET METADATA ===\")\n",
    "    \n",
    "    dataset_metadata = {\n",
    "        \"dataset_info\": {\n",
    "            \"name\": \"MET Museum Textiles and Tapestries Collection\",\n",
    "            \"version\": \"2.0 - CORRECTED\",\n",
    "            \"created_date\": timestamp,\n",
    "            \"description\": \"Complete collection of textile and tapestry objects from the Metropolitan Museum of Art\",\n",
    "            \"success_rate_percent\": round(total_downloaded/total_expected*100, 3),\n",
    "            \"api_calls_made\": \"~35,000\",\n",
    "            \"download_duration\": \"~8 hours\"\n",
    "        },\n",
    "        \"api_expected_counts\": {\n",
    "            \"textiles_from_api\": expected_textile_count,\n",
    "            \"tapestries_unique_from_api\": expected_tapestry_count,\n",
    "            \"total_expected\": total_expected\n",
    "        },\n",
    "        \"download_results\": {\n",
    "            \"textiles\": {\n",
    "                \"downloaded\": downloaded_textile_count,\n",
    "                \"with_images\": textile_images_count,\n",
    "                \"failed\": textile_failed_count,\n",
    "                \"success_rate\": round(downloaded_textile_count/expected_textile_count*100, 3)\n",
    "            },\n",
    "            \"tapestries\": {\n",
    "                \"downloaded\": downloaded_tapestry_count,\n",
    "                \"with_images\": tapestry_images_count,\n",
    "                \"failed\": tapestry_failed_count,\n",
    "                \"success_rate\": round(downloaded_tapestry_count/expected_tapestry_count*100, 3) if expected_tapestry_count > 0 else 100\n",
    "            },\n",
    "            \"totals\": {\n",
    "                \"total_downloaded\": total_downloaded,\n",
    "                \"total_with_images\": total_with_images,\n",
    "                \"total_failed\": total_failed,\n",
    "                \"overall_success_rate\": round(total_downloaded/total_expected*100, 3)\n",
    "            }\n",
    "        },\n",
    "        \"final_research_dataset\": {\n",
    "            \"description\": \"Objects with images only - ready for research\",\n",
    "            \"total_objects\": len(all_objects_with_images),\n",
    "            \"textiles_with_images\": textile_images_count,\n",
    "            \"tapestries_with_images\": tapestry_images_count,\n",
    "            \"filename\": f\"ALL_TEXTILES_AND_TAPESTRIES_WITH_IMAGES_{timestamp}.json\"\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_file = f\"{final_dir}/metadata/dataset_metadata_{timestamp}.json\"\n",
    "    with open(metadata_file, 'w') as f:\n",
    "        json.dump(dataset_metadata, f, indent=2)\n",
    "    print(f\"  📊 {metadata_file}: Complete metadata saved\")\n",
    "    \n",
    "    # 10. CREATE UPDATED README\n",
    "    readme_content = f\"\"\"# MET Museum Textiles and Tapestries Dataset (CORRECTED)\n",
    "\n",
    "## Overview\n",
    "This dataset contains textile and tapestry objects from the Metropolitan Museum of Art, collected via their public API.\n",
    "\n",
    "## Success Rate: {total_downloaded/total_expected*100:.3f}%\n",
    "\n",
    "## API Expected vs Downloaded\n",
    "- **Textiles**: {downloaded_textile_count:,} objects ({textile_images_count:,} with images) out of {expected_textile_count:,} expected\n",
    "- **Tapestries**: {downloaded_tapestry_count} objects ({tapestry_images_count} with images) out of {expected_tapestry_count} expected  \n",
    "- **Failed Downloads**: {total_failed} objects (243 textiles + {tapestry_failed_count} tapestries)\n",
    "\n",
    "## 🎯 FINAL RESEARCH DATASET\n",
    "**{len(all_objects_with_images):,} objects with images** - This is your main research dataset!\n",
    "- Located in: `objects_with_images_only/ALL_TEXTILES_AND_TAPESTRIES_WITH_IMAGES_{timestamp}.json`\n",
    "\n",
    "## Directory Structure\n",
    "```\n",
    "FINAL_CORRECTED_MET_TEXTILES_DATASET/\n",
    "├── all_objects/                    # Complete collections (all downloaded objects)\n",
    "├── objects_with_images_only/       # 🎯 RESEARCH READY - Objects with images only\n",
    "├── id_lists/                       # Object ID lists for reference\n",
    "├── metadata/                       # Dataset documentation\n",
    "└── README.md                       # This file\n",
    "```\n",
    "\n",
    "## Detailed Statistics\n",
    "- **Total Expected from API**: {total_expected:,} objects\n",
    "- **Total Successfully Downloaded**: {total_downloaded:,} objects\n",
    "- **Total with Images**: {total_with_images:,} objects\n",
    "- **Total Failed**: {total_failed} objects\n",
    "- **Overall Success Rate**: {total_downloaded/total_expected*100:.3f}%\n",
    "\n",
    "### Breakdown by Category\n",
    "- **Textiles**: {downloaded_textile_count:,}/{expected_textile_count:,} ({downloaded_textile_count/expected_textile_count*100:.1f}% success)\n",
    "- **Tapestries**: {downloaded_tapestry_count}/{expected_tapestry_count} ({downloaded_tapestry_count/expected_tapestry_count*100:.1f}% success)\n",
    "- **Intersections**: {len(intersection_ids)} objects appear in both categories\n",
    "\n",
    "## Files Created: {timestamp}\n",
    "\n",
    "### 🎯 RESEARCH READY (Images Only)\n",
    "- `ALL_TEXTILES_AND_TAPESTRIES_WITH_IMAGES_{timestamp}.json` - **FINAL RESEARCH DATASET**\n",
    "- `textiles_with_images_{timestamp}.json` - Textiles with images only\n",
    "- `tapestries_with_images_{timestamp}.json` - Tapestries with images only\n",
    "\n",
    "### Complete Collections (All Downloaded)\n",
    "- `complete_textiles_{timestamp}.json` - All downloaded textiles\n",
    "- `complete_tapestries_{timestamp}.json` - All downloaded tapestries\n",
    "- `textiles_tapestries_intersection_{timestamp}.json` - Objects in both categories\n",
    "\n",
    "### ID Lists (For Reference)\n",
    "- `all_objects_with_images_ids_{timestamp}.json` - IDs of final research dataset\n",
    "- `textile_object_ids_{timestamp}.json` - All textile IDs\n",
    "- `tapestry_object_ids_{timestamp}.json` - All tapestry IDs\n",
    "- `failed_object_ids_{timestamp}.json` - Failed download IDs\n",
    "\n",
    "## Usage Notes\n",
    "- **Use the `objects_with_images_only/` folder for research** - these objects have downloadable images\n",
    "- Each object contains full MET API metadata\n",
    "- Failed objects are mostly confirmed 404s (no longer exist in MET collection)\n",
    "- Image URLs in `primaryImage` field can be downloaded directly\n",
    "\n",
    "## Data Quality\n",
    "- {total_downloaded/total_expected*100:.3f}% success rate from API\n",
    "- All duplicates removed\n",
    "- Complete provenance tracking\n",
    "- Ready for academic research\n",
    "\n",
    "**🎯 Your final research dataset: {len(all_objects_with_images):,} objects with images!**\n",
    "\n",
    "Generated: {datetime.now()}\n",
    "\"\"\"\n",
    "    \n",
    "    readme_file = f\"{final_dir}/README.md\"\n",
    "    with open(readme_file, 'w') as f:\n",
    "        f.write(readme_content)\n",
    "    print(f\"  📝 {readme_file}: Documentation created\")\n",
    "    \n",
    "    # 11. FINAL CELEBRATION\n",
    "    print(f\"\\n🎉🎉🎉 === CORRECTED DATASET FINALIZATION COMPLETE === 🎉🎉🎉\")\n",
    "    print(f\"📁 Final directory: {final_dir}\")\n",
    "    print(f\"📊 API Expected: {total_expected:,} objects\")\n",
    "    print(f\"✅ Successfully Downloaded: {total_downloaded:,} objects\")\n",
    "    print(f\"🖼️  Objects with Images: {len(all_objects_with_images):,} objects\")\n",
    "    print(f\"❌ Failed: {total_failed} objects\")\n",
    "    print(f\"🎯 Success Rate: {total_downloaded/total_expected*100:.3f}%\")\n",
    "    print(f\"\\n🏆 YOUR FINAL RESEARCH DATASET: {len(all_objects_with_images):,} OBJECTS WITH IMAGES!\")\n",
    "    print(f\"📂 Located in: objects_with_images_only/ALL_TEXTILES_AND_TAPESTRIES_WITH_IMAGES_{timestamp}.json\")\n",
    "    \n",
    "    return {\n",
    "        \"final_directory\": final_dir,\n",
    "        \"total_expected\": total_expected,\n",
    "        \"total_downloaded\": total_downloaded,\n",
    "        \"total_with_images\": len(all_objects_with_images),\n",
    "        \"total_failed\": total_failed,\n",
    "        \"success_rate\": total_downloaded/total_expected*100,\n",
    "        \"final_research_dataset_size\": len(all_objects_with_images)\n",
    "    }\n",
    "\n",
    "# Run the corrected finalization\n",
    "corrected_stats = create_final_corrected_dataset()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
